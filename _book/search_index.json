[
["aggregate-loss-models.html", "Loss Data Analytics Chapter 1 Aggregate Loss Models 1.1 Introduction 1.2 Individual Risk Model 1.3 Collective Risk Model 1.4 Computing the Aggregate Claims Distribution 1.5 Effects of Coverage Modifications", " Loss Data Analytics An open text authored by the Actuarial Community 2018-05-19 Chapter 1 Aggregate Loss Models Chapter Preview. This chapter introduces probability models for describing the aggregate claims that arise from a portfolio of insurance contracts. We presents two standard modeling approaches, the individual risk model and the collective risk model. Further, we discuss strategies for computing the distribution of the aggregate claims. Finally, we examine the effects of individual policy modifications on the aggregate loss distribution. 1.1 Introduction The objective of this chapter is to build a probability model to describe the aggregate claims by an insurance system occurring in a fixed time period. The insurance system could be a single policy, a group insurance contract, a business line, or an entire book of an insurance’s business. In the chapter, aggregate claims refers to either the number or the amount of claims from a portfolio of insurance contracts. However, the modeling framework is readily to apply in the more general setup. Consider an insurance portfolio of \\(n\\) individual contracts, and let \\(S\\) denote the aggregate losses of the portfolio in a given time period. There are two approaches to modeling the aggregate losses \\(S\\), the individual risk model and the collective risk model. The individual risk model emphasizes the loss from each individual contract and represents the aggregate losses as: \\[\\begin{align*} S=X_1 +X_2 +\\cdots+X_n, \\end{align*}\\] where \\(X_i~(i=1,\\ldots,n)\\) is interpreted as the loss amount from the \\(i\\)th contract. It is worth stressing that \\(n\\) denotes the number of contracts in the portfolio and thus is a fixed number rather than a random variable. For the individual risk model, one usually assumes \\(X_{i}\\)’s are independent, i.e., \\(X_{i}\\perp X_{j}\\) \\(\\forall\\) \\(i,j\\). Because of different contract features such as coverage and exposure, \\(X_{i}\\)’s are not necessarily identically distributed. A notable feature of the distribution of each \\(X_i\\) is the probability mass at zero corresponding to the event of no claims. The collective risk model represents the aggregate losses in terms of a frequency distribution and a severity distribution: \\[\\begin{align*} S=X_1 +X_2 +\\cdots+X_N. \\end{align*}\\] Here one thinks of a random number of claims \\(N\\) that may represent either the number of losses or the number of payments. In contrast, in the individual risk model, we use a fixed number of contracts \\(n\\). We think of \\(X_1, X_2, \\ldots, X_N\\) as representing the amount of each loss. Each loss may or may not corresponding to a unique contract. For instance, there may be multiple claims arising from a single contract. It is natural to think about \\(X_i&gt;0\\) because if \\(X_i=0\\) then no claim has occurred. Typically we assume that conditional on \\(N=n\\), \\(X_{1},X_{2},\\cdots ,X_{n}\\) are i.i.d. random variables. The distribution of \\(N\\) is known as the frequency distribution, and the common distribution of \\(X\\) is known as the severity distribution. We further assume \\(N\\) and \\(X\\) are independent. With the collective risk model, we may decompose the aggregate losses into the frequency (\\(N\\)) process and the severity (\\(X\\)) process. This flexibility allows the analyst to comment on these two separate components. For example, sales growth due to lower underwriting standards could lead to higher frequency of losses but might not affect severity. Similarly, inflation or other economic forces could have an impact on severity but not on frequency. 1.2 Individual Risk Model As noted earlier, for the individual risk model, we think of \\(X_i\\) as the loss from \\(i^{th}\\) contract and interpret \\[\\begin{eqnarray*} S_n=X_1 +X_2 +\\cdots+X_n \\end{eqnarray*}\\] to be the aggregate loss from all contracts in a portfolio or group of contracts. Under the independence assumption on \\(X_i&#39;s\\), it is straightforward to show \\[\\begin{align*} {\\rm E}(S_n) &amp;= \\sum_{i=1}^{n} {\\rm E}(X_i),~~~~ {\\rm Var}(S_n) = \\sum_{i=1}^{n} {\\rm Var}(X_i)\\\\ P_{S_n}(z) &amp;= \\prod_{i=1}^{n}P_{X_i}(z), ~~~~ M_{S_n}(t) = \\prod_{i=1}^{n}M_{X_i}(t) \\\\ \\end{align*}\\] where \\(P_S(\\cdot)\\) and \\(M_S(\\cdot)\\) are probability generating function and moment generating function of \\(S\\), respectively. The distribution of each \\(X_i\\) contains mass at zero, corresponding to the event of no claim. One strategy to incorporate the zero mass in the distribution is using the two-part framework: \\[\\begin{align*} X_i = I_i\\times B_i = \\left\\{\\begin{array}{ll} 0 &amp; I_i=0 \\\\ B_i &amp; I_i=1 \\end{array} \\right. \\end{align*}\\] Here \\(I_i\\) is a Bernoulli variable indicating whether or not a loss occurs for the \\(i\\)th contract, and \\(B_i\\), a r.v. with nonnegative support, represents the amount of losses of the contract given loss occurrence. Assume that \\(I_1 ,\\ldots,I_n ,B_1 ,\\ldots,B_n\\) are mutually independent. Denote \\({\\rm Pr} (I_i =1)=q_i\\), \\(\\mu_i={\\rm E}(B_i)\\), and \\(\\sigma_i^2={\\rm Var}(B_i)\\). One can show \\[\\begin{align*} \\mathrm{E}(S_n)&amp; =\\sum_{i=1}^n ~q_i ~\\mu _j \\\\ \\mathrm{Var}(S_n) &amp; =\\sum_{i=1}^n \\left( q_i \\sigma _i^2+q_i (1-q_j)\\mu_i^2 \\right)\\\\ P_{S_n}(z) &amp; =\\sum_{i=1}^n \\left( 1-q_i+q_i P_{B_i}(z) \\right)\\\\ M_{S_n}(t) &amp; =\\sum_{i=1}^n \\left( 1-q_i+q_i M_{B_i}(t) \\right) \\end{align*}\\] A special case of the above model is when \\(B_i\\) follows a degenerate distribution with \\(\\mu_i=b_i\\) and \\(\\sigma^2_i=0\\). One example is term life insurance or a pure endowment insurance where \\(b_i\\) represents the amount of insurance of the \\(i\\)th contract. Another strategy to accommodate zero mass in the distribution of \\(X_i\\) is a collective risk model, i.e. \\(X_i=Z_{i1}+\\cdots+Z_{iN_i}\\) where \\(X_i=0\\) when \\(N_i=0\\). The collective risk model will be discussed in detail in the next section. Example. Course 3, May 2000, 19. An insurance company sold 300 fire insurance policies as follows: \\[\\begin{matrix} \\begin{array}{c c c} \\hline \\text{Number of} &amp; \\text{Policy} &amp; \\text{Probability of}\\\\ \\text{Policies} &amp; \\text{Maximum} &amp; \\text{Claim Per Policy}\\\\ \\hline 100 &amp; 400 &amp; 0.05\\\\ 200 &amp; 300 &amp; 0.06\\\\ \\hline \\end{array} \\end{matrix}\\] You are given: (i) The claim amount for each policy is uniformly distributed between \\(0\\) and the policy maximum. (ii) The probability of more than one claim per policy is \\(0\\). (iii) Claim occurrences are independent. Calculate the mean \\(\\mathrm{E~}S_n\\) and variance \\(\\mathrm{Var~}S_n\\) of the aggregate claims. How would these results change if every claim is equal to the policy maximum? Solution. The aggregate claims are \\(S_{300} = X_1+\\cdots+X_{300}\\). Policy claims amounts are uniformly distributed on \\((0,PolMax)\\), so the mean claim amount is \\(PolMax/2\\) and the variance is\\(PolMax^2/12\\). Thus, for policy \\(i=1,...,300\\), we have \\[\\begin{matrix} \\begin{array}{ccccc} \\hline \\text{Number of} &amp; \\text{Policy} &amp; \\text{Probability of} &amp; \\text{Mean} &amp; \\text{Variance}\\\\ \\text{Policies} &amp; \\text{Maximum} &amp; \\text{Claim Per Policy} &amp; \\text{Amount} &amp; \\text{Amount}\\\\ &amp; &amp; (q_{i}) &amp; (\\mu_{i}) &amp; (\\sigma_{i}^2) \\\\ \\hline 100 &amp; 400 &amp; 0.05 &amp; 200 &amp; 400^2/12\\\\ 200 &amp; 300 &amp; 0.06 &amp; 150 &amp; 300^2/12 \\\\ \\hline \\end{array} \\end{matrix}\\] The mean of the aggregate claims is \\[\\mathrm{E~} S_{300} = \\sum_{i=1}^{300} q_i \\mu_i = 100\\left\\{0.05(200)\\right\\} + 200\\left\\{0.06 (150) \\right\\} = 1,000+1,824 = 2,824\\] The variance of the aggregate claims is \\[\\begin{eqnarray*} \\mathrm{Var~}S_{300} &amp;=&amp; \\sum_{i=1}^{300} \\left( q_i \\sigma _i^2+q_i (1-q_i )\\mu_i^2 \\right) \\\\ &amp;=&amp; 100\\left\\{ 0.05 \\left(\\frac{400^2}{12}\\right) +0.05 (1-0.05 )200^2 \\right\\}+ 200\\left\\{ 0.06 \\left(\\frac{300^2}{12}\\right) +0.06 (1-0.06 )150^2 \\right\\}\\\\ &amp;=&amp; 600,466.67 . \\end{eqnarray*}\\] \\(\\Box\\) Follow-Up. Now suppose everybody receives the policy maximum if a claim occurs. What is the expected aggregate loss and variance of the aggregate loss? Each policy claim amount \\(B_i\\) is now fixed at \\(PolMax\\) instead of random, so \\(\\sigma_i^2 = \\mathrm{Var~} B_i = 0\\) and \\(\\mu_i = PolMax\\). \\[\\begin{align*} \\mathrm{E~}S^X &amp;= \\sum_{i=1}^{300} q_i \\mu_i = 100 \\left\\{0.05(400) \\right\\} + 200 \\left\\{ 0.06(300) \\right\\} = 5,648 \\end{align*}\\] \\[\\begin{align*} \\mathrm{Var~}S^X &amp;= \\sum_{i=1}^{300} \\left( q_i \\sigma _i^2+q_i (1-q_i )\\mu_i^2 \\right) = \\sum_{i=1}^{300} \\left( q_i (1-q_i) \\mu_i^2 \\right) \\\\ &amp;= 100 \\left\\{(0.05) (1-0.05) 400^2\\right\\} + 200 \\left\\{(0.06) (1-0.06)300^2\\right\\} \\\\ &amp;= 76,000 + 101,520 = 177,520 \\end{align*}\\] \\(\\Box\\) The individual risk model can also be used for claim frequency. If \\(X_i\\) denotes the number of claims from the \\(i\\)th contract, and \\(S_n\\) is interpreted as the total number of claims from the portfolio. In this case, the above two-part framework still applies. Assume \\(X_i\\) belongs to the \\((a,b,0)\\) class with pmf denoted by \\(p_{ik}\\). Let \\(X_i^{T}\\) denote the associated zero-truncated distribution in the \\((a,b,1)\\) class with the pmf \\(p_{ik}^T=p_{ik}/(1-p_{i0})\\) for \\(k=1,2,\\ldots\\). Using the relationship between their generating functions: \\[\\begin{align*} P_{X_i}(z) = p_{i0} +(1-p_{i0}) P_{X_i^{T}}(z), \\end{align*}\\] we can write \\(X_i=I_i\\times B_i\\) with \\(q_i={\\rm Pr}(I_i=1)={\\rm Pr}(X_i&gt;0)=1-p_{i0}\\) and \\(B_i=X_i^T\\). Example.An insurance company sold a portfolio of 100 independent homeowners insurance policies, each of which has claim frequency following a zero-modified Poisson distribution, as follows: \\[\\begin{matrix} \\begin{array}{cccc} \\hline \\text{Type of} &amp; \\text{Number of} &amp; \\text{Probability of} &amp; \\lambda \\\\ \\text{Policy} &amp; \\text{Policies} &amp; \\text{At Least 1 Claim} &amp; \\\\ \\hline \\text{Low-risk} &amp; 40 &amp; 0.03 &amp; 1 \\\\ \\text{High-risk} &amp; 60 &amp; 0.05 &amp; 2 \\\\ \\hline \\end{array} \\end{matrix}\\] Find the expected value and variance of the claim frequency for the entire portfolio. Solution. For each policy, we can write the zero-modified Poisson claim frequency \\(N_i\\) as \\(N_i = I_i \\times B_i\\), where \\[q_i = \\Pr(I_i = 1) = \\Pr(N_i &gt; 0) = 1-p_{i0}\\] For the low-risk policies, we have \\(q_i = 0.03\\) and for the high-risk policies, we have \\(q_i=0.05\\). Further, \\(B_i = N_i^T\\), the zero-truncated version of \\(N_i\\). Thus, we have \\[\\begin{align*} \\mu_i &amp;={\\rm E}(B_i) = {\\rm E}(N_i^T) = \\frac{\\lambda}{1-e^{-\\lambda}} \\\\ \\sigma_i^2 &amp;={\\rm Var}(B_i) = {\\rm Var}(N_i^T) = \\frac{\\lambda [1-(\\lambda+1)e^{-\\lambda}]}{(1-e^{-\\lambda})^2} \\end{align*}\\] Let the portfolio claim frequency be \\(S_n = \\sum_{i=1}^n N_i\\). Using the formulas above, the expected claim frequency of the portfolio is \\[\\begin{align*} \\mathrm{E~} S_n &amp;= \\sum_{i=1}^{100} q_i \\mu_i \\\\ &amp; = 40\\left[0.03 \\left(\\frac{1}{1-e^{-1}} \\right) \\right] + 60 \\left[0.05 \\left( \\frac{2}{1-e^{-2}} \\right) \\right] \\\\ &amp;= 40(0.03)(1.5820) + 60(0.05)(2.3130) = 8.8375 \\end{align*}\\] The variance of the claim frequency of the portfolio is \\[\\begin{align*} \\mathrm{Var~}S_n &amp;= \\sum_{i=1}^{100} \\left( q_i \\sigma _i^2+q_i (1-q_i )\\mu_i^2 \\right) \\\\ &amp;= 40 \\left[ 0.03 \\left(\\frac{1-2e^{-1}}{(1-e^{-1})^2} \\right) + 0.03(0.97)(1.5820^2) \\right] + 60 \\left[0.05 \\left( \\frac{2[1-3e^{-2}]}{ (1-e^{-2})^2} \\right) + 0.05(0.95)(2.3130^2) \\right] \\\\ &amp;= 23.7214 \\end{align*}\\] Note that equivalently, we could have calculated the mean and variance of an individual policy directly using the relationship between the zero-modified and zero-truncated Poisson distributions. \\(\\Box\\) To understand the distribution of the aggregate loss, one could use central limit theorem to approximate the distribution of \\(S_n\\). Denote \\(\\mu_S={\\rm E}(S)\\) and \\(\\sigma^2_S={\\rm Var}(S)\\), the cdf of \\(S_n\\) is: \\[\\begin{align*} F_{S_n}(s)={\\rm Pr}({S_n}\\leq s) = \\Phi \\left(\\frac{s-\\mu_S}{\\sigma_S}\\right). \\end{align*}\\] Example. Course 3, May 2000, 19 (Follow-Up). As in the original example earlier, an insurance company sold 300 fire insurance policies, with claim amounts uniformly distributed between 0 and the policy maximum. Using the normal approximation, calculate the probability that the aggregate claim amount exceeds \\(\\$3,500\\). Solution. We have seen earlier that \\(\\mathrm{E~} S_{300}=2,824\\) and \\(\\mathrm{Var~}S_{300} = 600,466.67\\). Then \\[\\begin{align*} {\\rm Pr}(S_{300} &gt; 3,500) &amp;= 1 - {\\rm Pr}(S_{300} \\leq 3,500) \\\\ &amp;= 1- \\Phi \\left( \\frac{3,500-2,824}{\\sqrt{600,466.67}} \\right) = 1 - \\Phi \\left( 0.87237 \\right) \\\\ &amp;= 1 - 0.8085 = 0.1915 \\end{align*}\\] \\(\\Box\\) For small \\(n\\), the distribution of \\(S_n\\) is likely skewed, and the normal approximation would be a poor choice. To examine the aggregate loss distribution, we go back to the basics and first principles. Specifically, the distribution can be derived recursively. Define \\(S_k=X_1 + \\cdots + X_k, k=1,\\ldots,n\\), we have: For \\(k=1\\): \\[F_{S_1}(s) = {\\rm Pr}(S_1\\leq s) = {\\rm Pr}(X_1\\leq s) = F_{X_1}(s).\\] For \\(k=2,\\ldots,n\\), \\[\\begin{align*} F_{S_k}(s)&amp;={\\Pr}(X_1+\\cdots+X_k\\leq s) ={\\Pr}(S_{k-1}+X_k\\leq s) \\\\ &amp;={\\rm E}_{X_k}\\left[{\\rm Pr}(S_{k-1}\\leq s-X_k|X_k)\\right]= {\\rm E}_{X_k}\\left[F_{S_{k-1}}(s-X_k)\\right]. \\end{align*}\\] There are some simple cases where the \\(S_n\\) has a closed form. Examples include If \\(X_i\\sim N(\\mu_i,\\sigma_i^2)\\), then \\(S_n\\sim N(\\sum_{i=1}^{n}\\mu_i,\\sum_{i=1}^{n}\\sigma_i^2)\\) If \\(X_i\\sim Exponential(\\theta)\\), then \\(S_n\\sim Gamma(n,\\theta)\\) If \\(X_i\\sim Gamma(\\alpha_i,\\theta)\\), then \\(S_n\\sim Gamma(\\sum_{i=1}^n\\alpha_i,\\theta)\\) If \\(X_i\\sim Poisson(\\lambda_i)\\), then \\(S_n\\sim Poisson(\\sum_{i=1}^{n}\\lambda_i)\\) If \\(X_i\\sim Bin(q,m_i)\\), then \\(S_n\\sim Bin(q,\\sum_{i=1}^n m_i)\\) If \\(X_i\\sim Geometric(\\beta)\\), then \\(S_n\\sim NegBin(\\beta,n)\\) If \\(X_i\\sim NegBin(\\beta,r_i)\\), then \\(S_n\\sim NegBin(\\beta,\\sum_{i=1}^n r_i)\\) A special case is when \\(X_i&#39;s\\) are identically distributed. Let \\(F_X(x)={\\Pr}(X\\leq x)\\) be the common distribution of \\(X_i\\) \\((i=1,\\ldots,n)\\), we define \\[F^{*n}_X(x)={\\Pr}(X_1+\\cdots+X_n\\leq x)\\] the \\(n\\)-fold convolution of \\(F_X\\). Example: Gamma Distribution. For an easy case, assume that \\(X_i \\sim\\) gamma with parameters \\((\\alpha, \\theta)\\). As we know, the moment generating function (mgf) is \\(M_{X}(t) = (1 - \\theta t)^{- \\alpha}\\). Thus, the mgf of the sum \\(S_n = X_1 + \\cdots + X_n\\) is \\[\\begin{eqnarray*} M_{S_n}(t) = \\mathrm{E~} \\exp(t(X_1 + \\cdots + X_n)) = (1 - \\theta t)^{-n \\alpha} , \\end{eqnarray*}\\] Thus, \\(S_n\\) has a gamma distribution with parameters \\((n \\alpha, \\theta)\\). This makes it easy to compute \\(F^{\\ast n}(x) = \\Pr(S_n \\le x).\\) This property is known as ``closed under convolution’’. \\(\\Box\\) Example: Negative Binomial Distribution. Assume \\(X_i \\sim NegBin(\\beta, r_i)\\). The probability generating function (pgf) is \\(P_X(z) = \\left[1-\\beta(z-1) \\right]^{-r}\\). Thus, the pgf of the sum \\(S_n =X_1+\\cdots+X_n\\) is \\[\\begin{align*} P_{S_n}(z) &amp;= \\mathrm{E~}\\left[ z^{S_n} \\right] = \\mathrm{E~}\\left[ z^{X_1+\\cdots+X_n} \\right] = \\mathrm{E~}\\left[ z^{X_1} z^{X_2} \\cdots z^{X_n} \\right] = \\mathrm{E~}\\left[z^{X_1}\\right] \\cdots \\mathrm{E~}\\left[z^{X_n}\\right] \\\\ &amp;= \\prod_{i=1}^n P_{X_i}(z) = \\prod_{i=1}^n \\left[1-\\beta(z-1) \\right]^{-r_i} = \\left[1-\\beta(z-1) \\right]^{-\\sum_{i=1}^n r_i} \\end{align*}\\] Thus, \\(S_n\\) has a negative binomial distribution with parameters \\((\\beta, \\sum_{i=1}^n r_i)\\). \\(\\Box\\) More generally, we can compute \\(F^{\\ast n}\\) recursively. Begin the recursion at \\(n=1\\) using \\(F^{\\ast 1} \\left(x \\right) =F(x)\\). Next, for \\(n=2\\), we have \\[\\begin{eqnarray*} F^{\\ast 2} \\left(x \\right) &amp;=&amp; \\Pr(X_1 + X_2 \\le x) = \\mathrm{E}_{X_2} \\Pr(X_1 \\le x - X_2|X_2)\\\\ &amp;=&amp; \\mathrm{E}_X F(x - X)\\\\ &amp;=&amp; \\int_{0}^{x} F(x-y) f(y) dy \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\text{continuous}\\\\ &amp;=&amp; \\sum_{y \\le x} F(x-y) f(y) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\;\\text{discrete}\\\\ \\end {eqnarray*}\\] Recall \\(F(0) = 0\\). Similarly, let \\(S_n = X_1 + X_2 + \\cdots + X_n\\) \\[\\begin{eqnarray*} F^{\\ast n}\\left(x\\right) &amp;=&amp; \\Pr(S_n \\le x) = \\Pr(S_{n-1} + X_n \\le x)\\\\ &amp;=&amp;\\mathrm{E}_{X_n}\\Pr(S_{n-1} \\le x - X_n|X_n)\\\\ &amp;=&amp;\\mathrm{E}_X F^{\\ast(n-1)}(x - X)\\\\ &amp;=&amp; \\int_{0}^{x} F^{\\ast(n-1)}(x-y)f(y)dy \\:\\:\\:\\text{continuous}\\\\ &amp;=&amp; \\sum_{y \\le x} F^{\\ast(n-1)}(x-y)f(y) \\:\\:\\:\\:\\:\\:\\:\\:\\text{discrete}\\\\ \\end{eqnarray*}\\] Example. SOA Sample Question, 283 (modified). The annual number of doctor visits for each individual in a family of 4 has geometric distribution with mean 1.5. The annual numbers of visits for the family members are mutually independent. An insurance pays 100 per doctor visit beginning with the 4th visit per family. Calculate the probability that the family will receive an insurance payment this year. Solution. Let \\(X_i \\sim Geometric(\\beta=1.5)\\) be the number of doctor visits for one individual in the family and \\(S_4 = X_1 + X_2 + X_3 + X_4\\) be the number of doctor visits for the family. The sum of 4 independent geometric distributions each with mean 1.5 follows a negative binomial distribution, i.e. \\(S_4 \\sim NegBin(\\beta=1.5, r=4)\\). If the insurance pays 100 per visit beginning with the 4th visit for the family, then the family will not receive an insurance payment if they have less than 4 claims. This probability is \\[\\begin{align*} \\Pr(S_4 &lt; 4) &amp;= \\Pr(S_4 = 0) + \\Pr(S_4 = 1) + \\Pr(S_4 = 2) +\\Pr(S_4 = 3) \\\\ &amp;= (1+1.5)^{-4} + \\frac{4(1.5)}{(1+1.5)^5} + \\frac{4(5)(1.5^2)}{2(1+1.5)^6} + \\frac{4(5)(6)(1.5^3)}{3!(1+1.5)^7}\\\\ &amp;= 0.0256 + 0.0614 + 0.0922 + 0.1106 = 0.2898 \\end{align*}\\] \\(\\Box\\) 1.3 Collective Risk Model 1.3.1 Moments and Distribution Under the collective risk model \\(S=X_1+\\cdots+X_N\\), \\(\\{X_i\\}\\) are i.i.d., and independent of \\(N\\). Let \\(\\mu = {\\rm E}\\left( X_{i}\\right)\\) and \\(\\sigma ^{2} = {\\rm Var}\\left(X_{i}\\right)\\) \\(\\forall\\) \\(i\\). Using the law of iterated expectations, the mean is \\[\\begin{eqnarray*} {\\rm E}(S)={\\rm E}_N[{\\rm E}_S(S|N)] = {\\rm E}_N[N\\mu] = \\mu {\\rm E}(N). \\end{eqnarray*}\\] Using the law of total variation, the variance is \\[\\begin{align*} {\\rm Var}(S) &amp;= {\\rm E}_N[{\\rm Var}_S(S|N)] + {\\rm Var}_N[{\\rm E}_S(S|N)] \\\\ &amp;={\\rm E}_N[N\\sigma^2] + {\\rm Var}_N[N\\mu] \\\\ &amp;=\\sigma^2{\\rm E}[N] + \\mu^2{\\rm Var}[N] \\end{align*}\\] Special Case: Poisson Distributed Frequency. If \\(N \\sim Poisson (\\lambda)\\), then \\[\\begin{eqnarray*} \\mathrm{E~}N &amp;=&amp; \\mathrm{Var~}N = \\lambda\\\\ \\mathrm{Var~}S &amp;=&amp; \\lambda (\\sigma^2 + \\mu^2) = \\lambda ~\\mathrm{E~} X^2 . \\end{eqnarray*}\\] Example. Course 3, May 2001, 36. The number of accidents follows a Poisson distribution with mean 12. Each accident generates 1, 2, or 3 claimants with probabilities 1/2, 1/3, and 1/6 respectively. Calculate the variance in the total number of claimants. Solution: \\[\\mathrm{E~}X^2 = 1^2 \\left( \\frac{1}{2}\\right) + 2^2\\left(\\frac{1}{3} \\right) + 3^2\\left(\\frac{1}{6}\\right) = \\frac{10}{3}\\] \\[\\mathrm{Var~}S = \\lambda \\ \\mathrm{E~}X^2 = 12\\left(\\frac{10}{3}\\right) = 40\\] Alternatively, using the general approach, \\(\\mathrm{Var~}S = \\sigma^2 \\mathrm{E~}N + \\mu^2 \\mathrm{Var~}N\\), where \\[\\mathrm{E~}N = \\mathrm{Var~}N = 12\\] \\[\\mu = \\mathrm{E~}X = 1\\left(\\frac{1}{2}\\right) + 2\\left(\\frac{1}{3}\\right) + 3\\left(\\frac{1}{6}\\right) = \\frac{5}{3}\\] \\[\\sigma^2 = \\mathrm{E~}X^2 - (\\mathrm{E~}X)^2 = \\frac{10}{3} - \\frac{25}{9} = \\frac{5}{9}\\] \\[\\Rightarrow \\ \\mathrm{Var~}S = \\left(\\frac{5}{9}\\right)\\left(12\\right) + \\left(\\frac{5}{3}\\right)^2\\left(12\\right) = 40\\] \\(\\Box\\) In general, the moments of \\(S\\) can be derived from its moment generating function (mgf). Because \\(\\{X_i\\}\\) are i.i.d., we denote the mgf of \\(X\\) as \\(M_{X}(t) = \\mathrm{E~}(e^{tX})\\). Using the law of iterated expectations, the mgf of \\(S\\) is \\[\\begin{eqnarray*} M_{S}(t) &amp;=&amp; \\mathrm{E}(e^{St})=\\mathrm{E}[\\mathrm{E}(e^{St}|N)]\\\\ &amp;=&amp; \\mathrm{E~}[(M_{X}(t))^N] \\end{eqnarray*}\\] where we use the relation \\(\\mathrm{E}[e^{t(X_1+\\cdots+X_n)}] = \\mathrm{E}(e^{tX_1})\\cdots\\mathrm{E}(e^{tX_n}) = (M_{X}(t))^n\\). Now, recall that the probability generating function (pgf) of \\(N\\) is \\(P(z) = \\mathrm{E}(z^N)\\). Denote \\(M_{X}(t)=z\\), it is shown \\[\\begin{eqnarray*} M_{S}(t) = \\mathrm{E~}(z^N) = P_{N}(z) = P_{N}[M_{X}(t)]. \\end{eqnarray*}\\] Similarly, if \\(S\\) a discrete, one can show the pgf of \\(S\\) is: \\[\\begin{eqnarray*} P_{S}(z) = P_{N}[P_{X}(z)]. \\end{eqnarray*}\\] To get \\(\\mathrm{E~}S = M_{S}&#39;(0)\\), we use the chain rule \\[ M_{S}&#39;(t) = \\frac{\\partial}{\\partial t} P_{N}(M_{X}(t)) = P_{N}&#39;(M_{X}(t)) M_{X}&#39;(t)\\\\ \\] and recall \\(M_{X}(0) = 1, M_{X}&#39;(0) = \\mathrm{E~}X = \\mu, P_{N}&#39;(1) = \\mathrm{E~}N\\). So, \\[\\begin{eqnarray*} \\mathrm{E~}S = M_{S}&#39;(0) = P_{N}&#39;(M_{X}&#39;(0)) M_{X}&#39;(0) = \\mu {\\rm E}(X) \\end{eqnarray*}\\] Similarly, one could use relation \\[ \\mathrm{E~}S^2 = M_{S}&#39;&#39;(0) \\] to get \\[\\mathrm{Var~}S = \\sigma^2 \\mathrm{E~}N + \\mu^2 \\mathrm{Var~}N.\\] Special Case. Poisson Frequency. Let \\(N \\sim Poisson (\\lambda)\\). Thus, the pgf of \\(N\\) is \\(P_N (z) =\\exp[\\lambda(z-1)]\\), and the mgf of \\(S\\) is \\[\\begin{eqnarray*} M_{S}(t) &amp;=&amp;\\exp[\\lambda(M_{X}(t) - 1)]. \\end{eqnarray*}\\] Taking derivatives yield \\[\\begin{eqnarray*} M_{S}(t) &amp;=&amp;\\exp(\\lambda(M_{X}(t) - 1))\\\\ M_{S}&#39;(t) &amp;=&amp;\\exp(\\lambda(M_{X}(t) - 1)) \\lambda M_{X}&#39;(t)\\\\ &amp;=&amp; M_{S}(t) \\lambda M_{X}&#39;(t)\\\\ M_{S}&#39;&#39;(t) &amp;=&amp; M_{S}(t) \\lambda M_{X}&#39;&#39;(t) + \\{M_{S}(t) \\lambda M_{X}&#39;(t)\\} \\lambda M_{X}&#39;(t) \\end{eqnarray*}\\] Evaluating these at \\(t=0\\) yields \\[\\begin{eqnarray*} M_{S}&#39;&#39;(0) &amp;=&amp; \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2\\\\ \\mathrm{Var~}S &amp;=&amp; \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2 - (\\lambda \\mu)^2\\\\ &amp;=&amp; \\lambda \\mathrm{E}(X^2). \\end{eqnarray*}\\] \\(\\Box\\) Example. Course 3, May 2001, 29. You are the producer of a television quiz show that gives cash prizes. The number of prizes, \\(N\\), and prize amount, \\(X\\), have the following distributions: \\[\\begin{matrix} \\begin{array}{ccccc}\\hline n &amp; \\Pr(N=n) &amp; &amp; x &amp; \\Pr(X=x)\\\\ \\hline 1 &amp; 0.8 &amp; &amp; 0 &amp; 0.2 \\\\ 2 &amp; 0.2 &amp; &amp; 100 &amp; 0.7 \\\\ &amp; &amp; &amp; 1000 &amp; 0.1\\\\\\hline \\end{array} \\end{matrix}\\] Your budget for prizes equals the expected aggregate cash prizes plus the standard deviation of aggregate cash prizes. Calculate your budget. Solution. We need to calculate the mean and standard deviation of the aggregate (sum) of cash prizes. The moments of the frequency distribution \\(N\\) are \\[\\begin{eqnarray*} \\mathrm{E~}N &amp;=&amp; 1 (0.8) + 2 (0.2) =1.2\\\\ \\mathrm{E~}N^2 &amp;=&amp; 1^2 (0.8) + 2^2 (0.2) =1.6\\\\ \\mathrm{Var~}N &amp;=&amp; \\mathrm{E~}N^2 - \\left( \\mathrm{E~}N \\right)^2= 0.16 \\end{eqnarray*}\\] The moments of the severity distribution \\(X\\) are \\[\\begin{eqnarray*} \\mathrm{E~}X &amp;=&amp; 0 (0.2) + 100 (0.7) + 1000 (0.1) = 170 = \\mu\\\\ \\mathrm{E~}X^2 &amp;=&amp; 0^2 (0.2) + 100^2 (0.7) + 1000^2 (0.1) = 107,000\\\\ \\mathrm{Var~}X &amp;=&amp; \\mathrm{E~}X^2 - \\left( \\mathrm{E~}X \\right)^2=78,100 = \\sigma^2 \\end{eqnarray*}\\] Thus, the mean and variance of the aggregate cash prize are \\[\\begin{eqnarray*} \\mathrm{E~}S &amp;=&amp; \\mu \\mathrm{E~}N = 170 (1.2) = 204 \\\\ \\mathrm{Var~}S &amp;=&amp; \\sigma^2 \\mathrm{E~}N + \\mu^2 \\mathrm{Var~}N\\\\ &amp;=&amp; 78,100 (1.2) + 170^2 (0.16) = 98,344 \\end{eqnarray*}\\] This gives the following required budget \\[\\begin{eqnarray*} Budget &amp;=&amp; \\mathrm{E~}S + \\sqrt{\\mathrm{Var~}S} \\\\ &amp;=&amp; 204 + \\sqrt{98,344} = 517.60 . \\end{eqnarray*}\\] \\(\\Box\\) The distribution of \\(S\\) is called a compound distribution, and it can be derived based on the convolution of \\(F_X\\) as follows: \\[\\begin{eqnarray*} F_{S}(s) &amp;=&amp; \\Pr \\left(X_1 + \\cdots + X_N \\le s \\right) \\\\ &amp;=&amp; \\mathrm{E} \\left[ \\Pr \\left(X_1 + \\cdots + X_N \\le s|N=n \\right) \\right]\\\\ &amp;=&amp; \\mathrm{E} \\left[ F_{X}^{\\ast N}(s) \\right] \\\\ &amp;=&amp; p_0 + \\sum_{n=1}^{\\infty }p_n F_{X}^{\\ast n}(s) \\end{eqnarray*}\\] Example. Course 3, Fall 2002, 36. The number of claims in a period has a geometric distribution with mean \\(4\\). The amount of each claim \\(X\\) follows \\(\\Pr(X=x) = 0.25, \\ x=1,2,3,4.\\) The number of claims and the claim amounts are independent. Let \\(S\\) denote the aggregate claim amount in the period. Calculate \\(F_{S}(3)\\) Solution. By definition, we have \\[\\begin{align*} F_{S}\\left(3 \\right) &amp;= {\\rm Pr}\\left(\\sum_{i=1}^N X_i \\leq 3\\right) = \\sum_{n=0}^\\infty {\\rm Pr}\\left(\\sum_{i=1}^n X_i\\leq 3|N=n\\right){\\rm Pr}(N=n) \\\\ &amp;= \\sum_n F^{\\ast n} \\left(3 \\right) p_n = \\sum_{n=0}^3 F^{\\ast n}(3) p_n \\\\ &amp;= p_0 + F^{\\ast 1}(3) \\ p_1 + F^{\\ast 2}(3) \\ p_2 + F^{\\ast 3}(3) \\ p_3 \\end{align*}\\] Because \\(N\\) has a geometric distribution with mean 4, we know that \\[\\begin{align*} p_n &amp;= \\frac{1}{1+\\beta} \\left(\\frac{\\beta}{1+ \\beta} \\right)^n = \\frac{1}{5} \\left(\\frac{4}{5} \\right)^n \\end{align*}\\] For the claim severity distribution, recursively, we have \\[\\begin{align*} F^{\\ast 1}(3) &amp;= \\Pr(X \\le 3) = \\frac{3}{4} \\\\ F^{\\ast 2}(3) &amp;= \\sum_{y \\le 3} F^{\\ast 1} (3-y) f(y) = F^{\\ast 1}(2)f(1) + F^{\\ast 1}(1)f(2) \\\\ &amp;= \\frac{1}{4}\\left[F^{\\ast 1} (2) + F^{\\ast 1}(1)\\right] = \\frac{1}{4}\\left[{\\rm Pr}(X\\leq 2) + {\\rm Pr}(X \\leq 1) \\right] \\\\ &amp;= \\frac{1}{4} \\left(\\frac{2}{4} + \\frac{1}{4} \\right) = \\frac{3}{16}\\\\ F^{\\ast 3}(3) &amp;= \\Pr(X_1+X_2 + X_3 \\le 3) = \\Pr(X_1=X_2=X_3=1) = \\left(\\frac{1}{4} \\right)^3 \\end{align*}\\] Notice that we did not need to recursively calculate \\(F^{\\ast 3}(3)\\) by recognizing that each \\(X \\in \\{1,2,3,4\\}\\), so the only way of obtaining \\(X_1+X_2+X_3 \\leq 3\\) is to have \\(X_1=X_2=X_3=1\\). Additionally, for \\(n \\geq 4\\), \\(F^{\\ast n} (3)=0\\) since it is impossible for the sum of 4 or more \\(X\\)’s to be less than 3. For \\(n=0\\), \\(F^{\\ast 0}(3) = 1\\) since the sum of 0 \\(X\\)’s is 0, which is always less than 3. Laying out the probabilities systematically, \\[\\begin {matrix} \\begin{array}{c c c c}\\hline x &amp; F^{\\ast 1}(x) &amp; F^{\\ast 2}(x) &amp; F^{\\ast 3}(x)\\\\ \\hline 0 &amp; &amp; &amp; \\\\ 1 &amp; \\frac{1}{4} &amp; 0 &amp; \\\\ 2 &amp; \\frac{2}{4} &amp; \\left( \\frac{1}{4} \\right)^2 &amp; \\\\ 3 &amp; \\frac{3}{4} &amp; \\frac{3}{16} &amp; \\left( \\frac{1}{4} \\right)^3 \\\\ \\hline \\end{array} \\end{matrix}\\] Finally, \\[\\begin{align*} F_{S}(3) &amp;= p_0 + F^{\\ast 1}(3) \\ p_1 + F^{\\ast 2}(3) \\ p_2 + F^{\\ast 3}(3) \\ p_3 \\\\ &amp;= \\frac{1}{5} + \\frac{3}{4}\\left(\\frac{4}{25} \\right) + \\frac{3}{16} \\left( \\frac{16}{125} \\right) + \\frac{1}{64} \\left( \\frac{64}{625}\\right) = 0.3456\\\\ \\end{align*}\\] \\(\\Box\\) When \\(\\mathrm{E}(N)\\), one may also use the central limit theorem to approximate the distribution of \\(S\\) as in the individual risk model. That is, \\(\\frac{S - \\mathrm{E}(S)}{\\sqrt{\\mathrm{Var}(S)}}\\) approximately follows \\(N(0,1)\\). Example. Exam 3, May 2000, 16. You are given: \\[\\begin{matrix} \\begin{array}{ c | c c } \\hline &amp; \\text{Mean} &amp; \\text{Standard Deviation}\\\\ \\hline \\text{Number of Claims} &amp; 8 &amp; 3\\\\ \\text{Individual Losses} &amp; 10,000 &amp; 3,937\\\\ \\hline \\end{array} \\end{matrix}\\] Using the normal approximation, determine the probability that the aggregate loss will exceed 150\\(\\%\\) of the expected loss: \\[\\begin{array}{ c c c c c} (A) \\Phi (1.25) &amp; (B) \\Phi (1.5) &amp; (C) 1-\\Phi (1.5) &amp; (D) 1-\\Phi (1.25) &amp; (E) 1.5 \\times \\Phi (1) \\end{array}\\] Solution. To use the normal approximation, we must first find the mean and variance of the aggregate loss \\(S\\) \\[\\begin{eqnarray*} \\mathrm{E~}S &amp;=&amp; \\mu \\ \\mathrm{E~}N = 10,000(8) = 80,000\\\\ \\mathrm{Var~}S &amp;=&amp; \\sigma^2 \\ \\mathrm{E~}N + \\mu^2 \\ \\mathrm{Var~}N\\\\ &amp;=&amp; 3937^2(8) + 10000^2 (3^2) = 1,023,999,752\\\\ \\sqrt{\\mathrm{Var~}S} &amp;=&amp; 31,999.996 \\approx 32,000 \\end{eqnarray*}\\] Then under the normal approximation, aggregate loss \\(S\\) is approximately normal with mean 80,000 and standard deviation 32,000. The probability that \\(S\\) will exceed 150\\(\\%\\) of the expected aggregate loss is therefore \\[\\begin{align*} \\Pr(S&gt;1.5 \\mathrm{E~}S) &amp;= \\Pr \\left( \\frac{S - \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~}S}} &gt; \\frac{1.5 \\mathrm{E~}S - \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~}S}} \\right) \\\\ &amp;= \\Pr \\left( N(0,1) &gt; \\frac{0.5 \\mathrm{E~}S}{\\sqrt{\\mathrm{Var~}S} } \\right) \\\\ &amp;= \\Pr \\left( N(0,1) &gt; \\frac{0.5(80,000)}{32,000} \\right) = \\Pr( N(0,1) &gt; 1.25) \\\\ &amp;= 1-\\Phi(1.25) = 0.1056 \\end{align*}\\] \\(\\Box\\) Example. Course 3, November 2000, 32. For an individual over \\(65\\): (i) The number of pharmacy claims is a Poisson random variable with mean \\(25\\). (ii) The amount of each pharmacy claim is uniformly distributed between \\(5\\) and \\(95\\). (iii) The amounts of the claims and the number of claims are mutually independent. Determine the probability that aggregate claims for this individual will exceed \\(2000\\) using the normal approximation: \\[\\begin{array}{ c c c c c} (A) 1-\\Phi (1.33) &amp; (B) 1-\\Phi (1.66) &amp; (C) 1-\\Phi (2.33) &amp; (D) 1-\\Phi (2.66) &amp; (E) 1-\\Phi (3.33) \\end{array}\\] Solution. We have claim frequency \\(N \\sim Poisson (\\lambda = 25)\\) and claim severity \\(X \\sim U \\left(5, 95 \\right)\\). To use the normal approximation, we need to find the mean and variance of the aggregate claims \\(S\\). Note \\[\\begin{matrix} \\begin{array}{lll} \\mathrm{E~} N = 25 &amp; &amp; \\mathrm{Var~} N = 25\\\\ \\mathrm{E~}X = \\frac{5+95}{2} = 50 = \\mu &amp; &amp; \\mathrm{Var~}X = \\frac{(95-5)^2}{12} = 675 = \\sigma^2\\\\ \\end{array} \\end{matrix}\\] Then for \\(S\\), \\[\\begin{eqnarray*} \\mathrm{E~}S &amp;=&amp; \\mu \\ \\mathrm{E~} N = 50(25) = 1,250\\\\ \\mathrm{Var~}S &amp;=&amp; \\sigma^2 \\ \\mathrm{E~}N + \\mu^2 \\ \\mathrm{Var~}N\\\\ &amp;=&amp; 675 (25) + 50^2 (25) = 79,375 \\end{eqnarray*}\\] Using the normal approximation, \\(S\\) is approximately normal with mean 1,250 and variance 79,375. The probability that \\(S\\) exceeds 2,000 is \\[\\begin{align*} \\Pr(S&gt;2,000) &amp;= \\Pr \\left(\\frac{S - \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~} S}} &gt; \\frac{2,000- \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~} S}} \\right) \\\\ &amp;= \\Pr\\left( N(0,1) &gt; \\frac{2,000-1,250}{\\sqrt{79,375}} \\right) \\\\ &amp;= \\Pr (N(0,1) &gt; 2.662) = 1-\\Phi(2.662) = 0.003884 \\end{align*}\\] \\(\\Box\\) 1.3.2 Stop-loss Insurance Insurance on the aggregate loss \\(S\\), subjected to a deductible \\(d\\), is called . The quantity \\[\\begin{eqnarray*} \\mathrm{E}[(S-d)_+] \\end{eqnarray*}\\] is known as the net stop-loss premium. To calculate the net stop-loss premium, we have \\[\\begin{eqnarray*} \\mathrm{E}(S-d)_+ &amp; =&amp; \\int_{d}^{\\infty} \\left(1-F_S(s) \\right) ds\\\\ &amp;=&amp; \\int_{d}^{\\infty}(s-d) f_{S}(s) ds ~~~~{\\rm continuous}\\\\ &amp;=&amp; \\sum_{s&gt;d}(s-d) f_{S}(s) ds ~~~~~{\\rm discrete}\\\\ &amp;=&amp; \\mathrm{E}(S) - \\mathrm{E}(S\\wedge d)\\\\ \\end{eqnarray*}\\] Example. Exam M, Fall 2005, 19. In a given week, the number of projects that require you to work overtime has a geometric distribution with \\(\\beta=2\\). For each project, the distribution of the number of overtime hours in the week is as follows: \\[\\begin{matrix} \\begin{array}{ccc} \\hline x &amp; &amp; f(x)\\\\ \\hline 5 &amp; &amp; 0.2 \\\\ 10 &amp; &amp; 0.3 \\\\ 20 &amp; &amp; 0.5\\\\ \\hline \\end{array} \\end{matrix}\\] The number of projects and the number of overtime hours are independent. You will get paid for overtime hours in excess of 15 hours in the week. Calculate the expected number of overtime hours for which you will get paid in the week. Solution. The number of projects in a week requiring overtime work has distribution \\(N \\sim Geometric(\\beta=2)\\), while the number of overtime hours worked per project has distribution \\(X\\) as described above. The aggregate number of overtime hours in a week is \\(S\\) and we are therefore looking for \\[\\mathrm{E~}(S-15)_+ = \\mathrm{E~}S - \\mathrm{E~}(S \\wedge 15).\\] To find \\(\\mathrm{E~}S = \\mathrm{E~}X \\ \\mathrm{E~}N\\), we have \\[\\begin{eqnarray*} \\mathrm{E~}X &amp;=&amp; 5(0.2) + 10(0.3)+ 20(0.5)= 14 \\\\ \\mathrm{E~}N &amp;=&amp; 2 \\\\ \\Rightarrow \\ \\mathrm{E~}S &amp;=&amp; \\mathrm{E~}X \\ \\mathrm{E~}N = 14(2) = 28 \\end{eqnarray*}\\] To find \\(\\mathrm{E~} (S \\wedge 15) = 0 \\Pr (S=0) + 5 \\Pr(S=5) + 10 \\Pr(S=10) + 15 \\Pr(S \\geq 15)\\), we have \\[\\begin{eqnarray*} \\Pr(S=0) &amp;=&amp; \\Pr(N=0) = \\frac{1}{1+\\beta} = \\frac{1}{3} \\\\ \\Pr(S=5) &amp;=&amp; \\Pr(X=5, \\ N=1) = 0.2 \\left(\\frac{2}{9} \\right)= \\frac{0.4}{9}\\\\ \\Pr(S=10) &amp;=&amp; \\Pr(X=10, \\ N=1) + \\Pr(X_1=X_2=5, N=2) \\\\ &amp;=&amp; 0.3 \\left(\\frac{2}{9} \\right) + (0.2)(0.2) \\left( \\frac{4}{27} \\right)= 0.0726 \\\\ \\Pr(S \\geq 15) &amp;=&amp; 1 - \\left(\\frac{1}{3} + \\frac{0.4}{9} + 0.0726 \\right) = 0.5496\\\\ \\Rightarrow \\mathrm{E~}(S \\wedge 15) &amp;=&amp; 0 \\Pr (S=0) + 5 \\Pr(S=5) + 10 \\Pr(S=10) + 15 \\Pr(S \\geq 15) \\\\ &amp;=&amp; 0 \\left( \\frac{1}{3} \\right) + 5 \\left( \\frac{0.4}{9} \\right) + 10 (0.0726) + 15 (0.5496) = 9.193\\\\ \\end{eqnarray*}\\] Therefore, \\[\\begin{eqnarray*} \\mathrm{E~}(S-15)_+ &amp;=&amp; \\mathrm{E~}S - \\mathrm{E~}(S \\wedge 15) \\\\ &amp;=&amp; 28 - 9.193 = 18.807 \\end{eqnarray*}\\] \\(\\Box\\) Recursive Net Stop-Loss Premium Calculation. For the discrete case, this can be computed recursively as \\[\\begin{eqnarray*} \\mathrm{E~}\\left[ S-(j+1)h \\right] _{+}=\\mathrm{E~}\\left[ ( S-jh )_{+} \\right] -h \\left( 1-F_S(jh) \\right) . \\end{eqnarray*}\\] This assumes that the support of \\(S\\) is equally spaced over units of \\(h\\). To establish this, we assume that \\(h=1\\). Now, on the left-hand side, we have \\(\\mathrm{E~}\\left[ S-(j+1) \\right] _{+}=\\mathrm{E~}S - \\mathrm{E~}S\\wedge (j+1)\\). We can write \\[\\begin{eqnarray*} \\mathrm{E~}S\\wedge (j+1) = \\sum_{x=0}^{j}xf_S(x) + (j+1)\\Pr(S \\ge j+1). \\end{eqnarray*}\\] Similarly \\[\\begin{eqnarray*} \\mathrm{E~}S\\wedge j = \\sum_{x=0}^{j}xf_S(x) + j\\Pr(S\\ge j+1). \\end{eqnarray*}\\] With these, expressions, we have \\[\\begin{eqnarray*} \\mathrm{E~}\\left[ S-(j+1) \\right] _{+} &amp;-&amp; \\mathrm{E~}\\left[ ( S-j )_{+} \\right] \\\\ &amp;=&amp;\\left\\{\\mathrm{E~}S - \\mathrm{E~}S\\wedge (j+1) \\right\\} -\\left\\{\\mathrm{E~}S - \\mathrm{E~}S\\wedge j \\right\\} \\\\ &amp;=&amp;\\left\\{ \\sum_{x=0}^{j}xf_S(x) + j\\Pr(S\\ge j+1) \\right\\} - \\left\\{ \\sum_{x=0}^{j}xf_S(x) + (j+1)\\Pr(S \\ge j+1) \\right\\} \\\\ &amp;=&amp; -\\Pr(S\\ge j+1) = -\\{1 - F_{S}(j)\\}, \\end{eqnarray*}\\] as required. \\(\\Box\\) Exercise. Exam M, Fall 2005, 19 - Continued. Recall that the goal of this question was to calculate \\(\\mathrm{E~}(S-15)_+\\). Note that the support of \\(S\\) is equally spaced over units of 5, so this question can also be done recursively, using steps of \\(h=5\\): Step 1: \\[\\begin{align*} \\mathrm{E~}(S-5)_+ &amp;= \\mathrm{E~}S - 5 [1-\\Pr(S \\leq 0) ]\\\\ %\\Pr (S\\geq 5) \\\\ &amp;= 28 - 5 \\left(1 - \\frac{1}{3}\\right) = \\frac{74}{3}=24.6667 \\end{align*}\\] Step 2: \\[\\begin{align*} \\mathrm{E~}(S-10)_+ &amp;= \\mathrm{E~}(S-5)_+ - 5 [1-\\Pr(S \\leq 5)]\\\\ %\\Pr (S\\ge 10) \\\\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\frac{1}{3} - \\frac{0.4}{9}\\right) = 21.555 \\end{align*}\\] Step 3: \\[\\begin{align*} \\mathrm{E~}(S-15)_+ &amp;= \\mathrm{E~}(S-10)_+ - 5 [1-\\Pr(S \\leq 10)] \\\\ %\\Pr (S\\ge 15) \\\\ &amp;= \\mathrm{E~}(S-10)_+ - 5\\Pr (S\\ge 15) \\\\ &amp;= 21.555 - 5 (0.5496) = 18.887 \\end{align*}\\] \\(\\Box\\) 1.3.3 Analytic Results There are few combinations of claim frequency and severity distributions that result in an easy-to-compute distribution for aggregate losses. This section gives some simple examples. Analysts view these examples as too simple to be used in practice. Example \\(\\#1\\) One has a closed-form expression for the aggregate loss distribution by assuming a geometric frequency distribution and an exponential severity distribution. Assume that claim count \\(N\\) is geometric with parameter \\(\\beta\\) such that \\(\\mathrm{E}(N)=\\beta\\), and that claim amount \\(X\\) is exponential with parameter \\(\\theta\\) such that \\(\\mathrm{E}(X)=\\theta\\). Recall that the pgf of \\(N\\) and the mgf of \\(X\\) are: \\[\\begin{align*} P_N (z) &amp;=\\frac{1}{1- \\beta (z-1)},\\\\ M_{X}(t) &amp;=\\frac{1}{1-\\theta t}. \\end{align*}\\] Thus, the mgf of aggregate loss \\(S\\) is \\[\\begin{eqnarray} M_{S}(t) &amp;=&amp; P_N [M_{X}(t)] = \\frac{1}{1 - \\beta \\left( \\frac{1}{1-\\theta t} + 1\\right)} \\nonumber\\\\ &amp;=&amp; 1+ \\frac{1}{1+\\beta} ([1-\\theta(1+\\beta)z]^{-1}-1)...(1)\\\\ &amp;=&amp; \\frac{1}{1+\\beta}(1) +\\frac{\\beta}{1+\\beta} \\left( \\frac{1}{1-\\theta (1+\\beta)t}\\right)...(2) \\end{eqnarray}\\] From (1), we note that \\(S\\) is equivalent to the compound distribution of \\(S=X^{*}_1+\\cdots+X^{*}_{N^{*}}\\), where \\(N^{*}\\) is a Bernoulli with mean \\(\\beta/(1+\\beta)\\) and \\(x^{*}\\) is an exponential with mean \\(\\theta(1+\\beta)\\). To see this, we examine the mgf of \\(S\\): \\[\\begin{align*} M_{S}(t) = P_N [M_{X}(t)] = P_{N^{*}} [M_{X^{*}}(t)], \\end{align*}\\] where \\[\\begin{align*} P_{N^*} (z) &amp;=1+ \\color{blue}{\\frac{\\beta}{1+ \\beta}} (z-1),\\\\ M_{X^*} (t) &amp;=\\frac{1}{1- {\\color{blue}{\\theta(1+\\beta)}} t}. \\end{align*}\\] From (2), we note that \\(S\\) is also equivalent to a 2-point mixture of 0 and \\(X^{*}\\). Specifically, \\[\\begin{eqnarray*} S &amp;=&amp; \\left\\{ \\begin{array}{cl} 0 &amp; {\\rm with~ probability ~Pr}(N^*=0) = 1/(1+\\beta) \\\\ Y^{*} &amp; {\\rm with~ probability ~Pr}(N^*=1) = \\beta/(1+\\beta) \\end{array} \\right.. \\end{eqnarray*}\\] The distribution function of \\(S\\) is: \\[\\begin{eqnarray*} \\Pr(S=0) &amp;=&amp; \\frac{1}{1+\\beta}\\\\ \\Pr(S&gt;s) &amp;=&amp; \\Pr(X^*&gt;s) =\\frac{\\beta}{1+\\beta} \\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) \\end{eqnarray*}\\] with pdf \\[\\begin{eqnarray*} f_{S}(s) = \\frac{\\beta}{\\theta (1+\\beta)^2}\\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right). \\end{eqnarray*}\\] \\(\\Box\\) Example \\(\\#2\\) Consider a collective risk model with an exponential severity and an arbitrary frequency distribution. Recall that if If \\(X_i\\sim Exponential(\\theta)\\), then the sum of i.i.d. exponential, \\(S_n=X_1+\\cdots+X_n\\), has a Gamma distribution, i.e. \\(S_n\\sim Gamma(n,\\theta)\\). This has cdf: \\[\\begin{eqnarray*} F_{X}^{\\ast n}(s) &amp;=&amp; \\Pr (S_n \\le s) = \\int_{0}^{s} \\frac{1}{\\Gamma(n)\\theta^n}s^{n-1}\\exp\\left(-\\frac{s}{\\theta}\\right) ds\\\\ &amp;=&amp; 1-\\sum_{j=0}^{n-1}\\frac{1}{j!}\\left( \\frac{s}{\\theta}\\right)^j e^{-s/\\theta } . \\end{eqnarray*}\\] The last equality is derived by integration by parts. For the aggregate loss distribution, we can interchange order of summations to get \\[\\begin{eqnarray*} F_{S}\\left(s\\right) &amp;=&amp; p_{0}+\\sum_{n=1}^{\\infty }p_n F_{X}^{\\ast n}\\left(s\\right)\\\\ &amp;=&amp; 1 - \\sum_{n=1}^{\\infty }p_n \\sum_{j=0}^{n-1}\\frac{1}{j!} \\left( \\frac{s}{\\theta}\\right)^j e^{-s/\\theta }\\\\ &amp;=&amp; 1-e^{-s/\\theta}\\sum_{j=0}^{\\infty} \\frac{1}{j!} \\left( \\frac{s}{\\theta} \\right)^j \\overline{P}_j \\end{eqnarray*}\\] where \\(\\overline{P}_j =p_{j+1}+p_{j+2}+\\cdots = \\Pr (N&gt;j),\\) the ``survival function’’ of the claims count distribution. 1.3.4 Tweedie Distribution In this section, we examine a particular compound distribution where the number of claims is a Poisson distribution and the amount of claims is a Gamma distribution. This specification leads to what is known as a Tweedie distribution. The Tweedie distribution has a mass probability at zero and a continuous component for positive values. Because of this feature, it is widely used in insurance claims modeling, where the zero mass is interpreted as no claims and the positive component as the amount of claims. Specifically, consider the collective risk model \\(S=X_1+\\cdots+X_N\\). Suppose that \\(N\\) has a Poisson distribution with mean \\(\\lambda\\), and each \\(X_i\\) has a Gamma distribution shape parameter \\(\\alpha\\) and scale parameter \\(\\gamma\\). The Tweedie distribution is derived as the Poisson sum of gamma variables. To understand the distribution of \\(S\\), we first examine the mass probability at zero. It is straightforward to see that the aggregate loss is zero when there is no claims occurred, thus: \\[f_S(0)={\\rm Pr}(S=0)= {\\rm Pr}(N=0)=e^{-\\lambda}.\\] In addition, one notes that that \\(S\\) conditional on \\(N_i=n\\), denoted by \\(S_n=X_1+\\cdots+X_n\\), follows a gamma distribution with shape \\(n\\alpha\\) and scale \\(\\gamma_i\\). Thus, for \\(s&gt;0\\), the density of a Tweedie distribution can be calculated as \\[\\begin{align*} f_S(s)&amp;=\\sum_{n=1}^{+\\infty} p_n f_{S_n}(s)\\\\ &amp;=\\sum_{n=1}^{\\infty}e^{-\\lambda_i}\\frac{(\\lambda_i)^n}{n!}\\frac{1}{\\gamma^{n\\alpha}}y^{n\\alpha-1}e^{-y\\gamma} \\end{align*}\\] Thus, the Tweedie distribution can be thought of a mixture of zero and a positive valued distribution, which makes it a convenient tool for modeling insurance claims and for calculating pure premiums. The mean and variance of the Tweedie compound Poisson model are: \\[{\\rm E} (S)=\\lambda\\frac{\\alpha}{\\gamma}~~~~{\\rm and}~~~~{\\rm Var} (S)=\\lambda\\frac{\\alpha(1+\\alpha)}{\\gamma^2}.\\] As another important feature, the Tweedie distribution is a special case of exponential dispersion models, a class of models used to describe the random component in generalized linear models. To see this, we consider the following reparameterizations: \\[\\begin{equation*} \\lambda=\\frac{\\mu^{2-p}}{\\phi(2-p)},~~~~\\alpha=\\frac{2-p}{p-1},~~~~\\gamma=\\phi(p-1)\\mu^{p-1} \\end{equation*}\\] With the above relationships, one can show that the distribution of \\(S\\) is \\[f_S(s)=\\exp\\left[\\frac{1}{\\phi}\\left(\\frac{-s}{(p-1)\\mu^{p-1}}-\\frac{\\mu^{2-p}}{2-p}\\right)+C(s;\\phi)\\right]\\] where \\[\\begin{equation*} C(s;\\phi/\\omega_i)=\\left\\{\\begin{array}{ll} \\displaystyle 0 &amp; {\\rm if}~ y=0 \\\\ \\displaystyle \\ln \\sum\\limits_{n\\ge 1} \\left\\{\\frac{(1/\\phi)^{1/(p-1)}y^{(2-p)/(p-1)}}{(2-p)(p-1)^{(2-p)/(p-1)}}\\right\\}^{n}\\frac{1}{n!\\Gamma(n(2-p)/(p-1))s} &amp; {\\rm if}~ y&gt;0 \\end{array}\\right. \\end{equation*}\\] Hence, the distribution of \\(S\\) belongs to the exponential family with parameters \\(\\mu\\), \\(\\phi\\), and \\(p\\in(1,2)\\), and we have \\[{\\rm E} (S)=\\mu~~~~{\\rm and}~~~~{\\rm Var} (S)=\\phi\\mu^{p}\\] It is also worth mentioning the two limiting cases of the Tweedie model: \\(p\\rightarrow 1\\) results in the Poisson distribution and \\(p\\rightarrow 2\\) results in the gamma distribution. The Tweedie compound Poisson model accommodates the situations in between. 1.4 Computing the Aggregate Claims Distribution Computing the distribution of aggregate losses is a difficult, yet important, problem. As we have seen, for both individual risk model and collective risk model, computing the distribution involves the evaluation of a \\(n\\)-fold convolution. To make the problem tractable, one strategy is to use a distribution that is easy to evaluate to approximate the aggregate loss distribution. For instance, normal distribution is a natural choice based on central limit theorem where parameters of the normal distribution can be estimated by matching the moments. This approach has its strength and limitations. The main advantage is the ease of computation. The disadvantage are: first, the size and direction of approximation error are unknown; second, the approximation may fail to capture some special features of the aggregate loss such as mass point at zero. This section discusses two practical approaches to computing the distribution of aggregate loss, the recursive method and the simulation. 1.4.1 Recursive Method The recursive method applies to compound models where the frequency component \\(N\\) belongs to either \\((a,b,0)\\) or \\((a,b,1)\\) class and the severity component \\(X\\) has a discrete distribution. For continuous \\(X\\), a common practice is to first discretize the severity distribution and then the recursive method is ready to apply. Assume that \\(N\\) is in the \\((a,b,1)\\) class so that \\(p_{k}=\\left( a+\\frac{b}{k} \\right) p_{k-1}, k = 2,3,\\ldots\\). Further assume that the support of \\(X\\) is \\(\\{0,1,\\ldots,m\\}\\), discrete and finite. Then, the probability function of \\(S\\) is: \\[\\begin{align*} f_{S}(s)&amp;=\\Pr (S=s) \\\\ &amp;=\\frac{1}{1-af_{X}(0)}\\left\\{ \\left[ p_1 -(a+b)p_{0}\\right] f_X (s)+\\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx}{s} \\right) f_X (x)f_{S}(s-x)\\right\\}. \\end{align*}\\] If \\(N\\) is in the \\((a,b,0)\\) class, then \\(p_1=(a+b)p_0\\) and so \\[ f_S(s)=\\frac{1}{1-af_X (0)}\\left\\{ \\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx }{s}\\right) f_X (x)f_{S}(s-x)\\right\\}. \\] Special Case: If \\(N \\sim\\) Poisson with mean \\(\\lambda\\), then \\(a=0\\) and \\(b=\\lambda\\), thus \\[ f_{S}(s)=\\frac{\\lambda }{s}\\left\\{ \\sum_{x=1}^{s \\wedge m} x f_X (x) f_S (s-x)\\right\\} . \\] Example. SOA Fall 2002, 36. The number of claims in a period \\(N\\) has a geometric distribution with mean 4. The amount of each claim \\(X\\) follows \\({\\rm Pr} (X = x) = 0.25\\), for \\(x = 1,2,3,4\\). The number of claims and the claim amount are independent. \\(S\\) is the aggregate claim amount in the period. Calculate \\(F_S(3)\\). Solution. The severity distribution \\(X\\) follows \\[f_X (x) = \\frac{1}{4}, \\ \\ x=1, 2, 3, 4.\\] The frequency distribution \\(N\\) is geometric with mean 4, which is a member of the \\((a,b,0)\\) class with \\(b=0\\), \\(a=\\frac{\\beta}{1+\\beta} = \\frac{4}{5}\\), and \\(p_0 = \\frac{1}{1+\\beta} = \\frac{1}{5}\\). Thus, we can use the recursive method \\[\\begin{eqnarray*} f_S (x) &amp;=&amp; 1 \\sum_{y=1}^{x\\wedge m} (a+0) f_X (y) f_S (x-y) \\\\ &amp;=&amp; \\frac{4}{5} \\sum_{y=1}^{x\\wedge m} f_X (y) f_S (x-y) \\end{eqnarray*}\\] Specifically, we have \\[\\begin{eqnarray*} f_S (0) &amp;=&amp; \\Pr(N=0) = p_0=\\frac{1}{5}\\\\ f_S (1) &amp;=&amp; \\frac{4}{5}\\sum_{y=1}^{1} f_X (y) f_S (1-y) = \\frac{4}{5} f_X(1) f_S(0)\\\\ &amp;=&amp; \\frac{4}{5}\\left( \\frac{1}{4}\\right)\\left(\\frac{1}{5} \\right) = \\frac{1}{25}\\\\ f_S (2) &amp;=&amp; \\frac{4}{5}\\sum_{y=1}^{2} f_X (y) f_S (2-y) = \\frac{4}{5} \\left[ f_X(1)f_S(1) + f_X(2) f_S(0) \\right] \\\\ &amp;=&amp; \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5}\\right) \\right] = \\frac{4}{5}\\left( \\frac{6}{100}\\right) = \\frac{6}{125}\\\\ f_S (3) &amp;=&amp; \\frac{4}{5} \\left[ f_X(1) f_S(2) + f_X(2)f_S(1) + f_X(3) f_S(0) \\right]\\\\ &amp;=&amp; \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5} + \\frac{6}{125}\\right) \\right] = \\frac{1}{5}\\left( \\frac{5+25+6}{125}\\right) = 0.0576\\\\ \\Rightarrow \\ F_S (3) &amp;=&amp; f_S (0)+f_S (1)+f_S (2)+f_S (2)+f_S (3) = 0.3456 \\end{eqnarray*}\\] \\(\\Box\\) 1.4.2 Simulation The distribution of aggregate loss can be evaluated using Monte Carlo simulation. The idea is one can calculate the empirical distribution of \\(S\\) using a random sample. Blow we summarize the simulation procedures for the aggregate loss models. Individual Risk Model \\(S_n=X_1+\\cdots+X_n\\) For each \\(X_i\\), \\(i=1,\\ldots,n\\), generate random sample of size \\(m\\), denoted by \\(x_{ij}~(j=1,\\ldots,m)\\); Calculate the aggregate loss \\(s_j=x_{1j}+\\ldots+x_{nj}\\) for \\(j=1,\\ldots,m\\); We obtain a random sample of \\(S\\), i.e. \\(\\{s_1,\\ldots,s_m\\}\\). Collective Risk Model \\(S=Y_1+\\cdots+Y_N\\) Generate the number of claims \\(n_j\\) from frequency distribution \\(N\\); Given \\(n_j\\), generate the amount of claims for each claim independently from \\(Y\\), denoted by \\(y_{1},\\ldots,y_{n_j}\\); Calculate the aggregate loss \\(s_j=y_{1j}+\\ldots+y_{n_j}\\); Repeat the above three steps for \\(j=1,\\ldots,m\\); We obtain a random sample of \\(S\\), i.e. \\(\\{s_1,\\ldots,s_m\\}\\). Given the random sample of \\(S\\), the empirical distribution can be calculated as \\[\\hat{F}_S(s)=\\frac{1}{m}\\sum_{i=1}^{m}I(s_i\\leq s),\\] where \\(I(\\cdot)\\) is an indicator function. The empirical distribution \\(\\hat{F}_S(s)\\) will converge to \\({F}_S(s)\\) almost surely as \\(m\\rightarrow \\infty\\). The above procedure assumes that the parameters of the frequency and severity distributions are known. In practice, one would need to estimate these parameters from the data. For instance, the assumptions in the collective risk model suggest a two-stage estimation where a model is developed for the number of claims \\(N\\) from the data on claim counts and a model is developed for the severity of claims \\(X\\) from the data on the amount of claims. 1.5 Effects of Coverage Modifications 1.5.1 Impact of Exposure on Frequency This section focuses on an individual risk model for claim counts. Consider the number of claims from a group of \\(n\\) policies: \\[S=X_1+\\cdots+X_n\\] where we assume \\(X_i\\) are i.i.d. representing the number of claims from policy \\(i\\). In this case, the exposure for the portfolio is \\(b\\) using policy as exposure base. The pgf of \\(S\\) is \\[\\begin{align*} P_S(z)&amp;={\\rm E}(z^S)={\\rm E}\\left(z^{\\sum_{i=1}^nX_i}\\right)\\\\ &amp;=\\prod_{i=1}^n{\\rm E}(z^{X_i})=[P_X(z)]^n \\end{align*}\\] Special Case Poisson. If \\(X_i\\sim Poisson(\\lambda)\\), its pgf is \\(P_X(z)=e^{\\lambda(z-1)}\\). Then the pgf of \\(S\\) is \\[P_S(z)=[e^{\\lambda(z-1)}]^n=e^{n\\lambda(z-1)}.\\] So \\(S\\sim Poisson(n\\lambda)\\). Special Case Negative binomial. If \\(X_i\\sim NegBin(\\beta,r)\\), its pgf is \\(P_X(z)=[1-\\beta(z-1)]^{-r}\\). Then the pgf of \\(S\\) is \\[P_S(z)=[[1-\\beta(z-1)]^{-r}]^n=[1-\\beta(z-1)]^{-nr}.\\] So \\(S\\sim NB(\\beta,nr)\\). Example. Assume that the number of claims for each vehicle is Poisson with mean \\(\\lambda\\). Given the following data on the observed number of claims for each household, calculate the MLE of \\(\\lambda\\). \\[\\begin{matrix} \\begin{array}{|c|c|c|} \\hline \\text{Household ID} &amp; \\text{Number of vehicles} &amp; \\text{Number of claims} \\\\ \\hline 1 &amp; 2 &amp; 0 \\\\ 2 &amp; 1 &amp; 2 \\\\ 3 &amp; 3 &amp; 2 \\\\ 4 &amp; 1 &amp; 0 \\\\ 5 &amp; 1 &amp; 1 \\\\ \\hline \\end{array} \\end{matrix}\\] Solution. Each of the 5 households has number of exposures \\(b_j\\) (number of vehicles) and number of claims \\(S_j\\), \\(j=1,...,5\\). Note for each household, the number of claims \\(S_j \\sim Poisson (b_j \\lambda)\\). The likelihood function is \\[\\begin{align*} L(\\lambda) &amp;= \\prod_{j=1}^5 \\Pr(S_j=s_j) = \\prod_{j=1}^5 \\frac{e^{-b_j\\lambda} (b_j \\lambda)^{s_j}}{s_j!} \\\\ &amp;= \\left(\\frac{e^{-2\\lambda} (2 \\lambda)^{0}}{0!} \\right) \\left(\\frac{e^{-1\\lambda} (1 \\lambda)^{2}}{2!} \\right) \\left(\\frac{e^{-3\\lambda} (3 \\lambda)^{2}}{2!} \\right) \\left(\\frac{e^{-1\\lambda} (1 \\lambda)^{0}}{0!} \\right) \\left(\\frac{e^{-1\\lambda} (1 \\lambda)^{1}}{1!} \\right) \\\\ &amp;\\propto e^{-8\\lambda} \\lambda^5 \\end{align*}\\] Taking the log-likehood, we have \\[\\begin{align*} l(\\lambda) = \\log L(\\lambda) = -8\\lambda + 5\\log(\\lambda) \\end{align*}\\] Setting the first derivative of the log-likehood to 0, we have \\[\\begin{align*} &amp;l&#39;(\\lambda) = -8 + \\frac{5}{\\lambda} = 0 \\\\ \\Rightarrow \\ &amp; 8 = \\frac{5}{\\hat{\\lambda}} \\ \\Rightarrow \\ \\hat{\\lambda} = \\frac{5}{8} \\end{align*}\\] \\(\\Box\\) If the exposure of the portfolio change from \\(n_1\\) to \\(n_2\\), we can establish the following relation between the aggregate claim counts: \\[P_{S_2}(z)=[P_X(z)]^{n_2}=[P_X(z)^{n_1}]^{n_2/n_1}=P_{S_1}(z)^{n_2/n_1}.\\] 1.5.2 Impact of Deductibles on Claim Frequency This section examine the effect of deductible on claim frequency. Intuitively, there will be fewer claims filed when a policy deductible is imposed because a loss below deductible might not result in a claim. Even if an insured does file a claim, this may not result in a payment by the policy, since the claim may be denied or the loss amount may ultimately be determined to be below deductible. Let \\(N^L\\) denote the number of loss (i.e. the number of claims with no deductible), and \\(N^P\\) denote the number of payments when a deductible \\(d\\) is imposed. Our goal is to identify the distribution of \\(N^P\\) given the distribution of \\(N^L\\). We show below that the relationship between \\(N^L\\) and \\(N^P\\) can be established within an aggregate risk model framework. Note that sometimes changes in deductible will affect policyholder behavior. We assume that this is not the case, i.e. the distribution of losses for both frequency and severity remain unchanged when the deductible changes. Given there are \\(N^L\\) losses, let \\(X_1,X_2\\ldots,X_{N^L}\\) be the associated amount of losses. For \\(j=1,\\ldots,N^L\\), define \\[\\begin{eqnarray*} I_j&amp;=&amp; \\left \\{ \\begin{array}{cc} 1 &amp; \\text{if} ~X_j&gt;d\\\\ 0 &amp; \\text{otherwise}\\\\ \\end{array} \\right.. \\end{eqnarray*}\\] Then we establish \\[N^P=I_1+I_2+\\cdots+I_{N_L}.\\] Note that conditioning on \\(N^L\\), the distribution of \\(N^P \\sim Binomial (N^L, v)\\), where \\(v=\\Pr(X&gt;d)\\). Thus, given \\(N^L\\), \\[\\begin{eqnarray*} \\mathrm{E}\\left(z^{N^P}|N^L\\right)&amp;=&amp;\\left[ 1+v(z-1)\\right]^{N^L} \\end{eqnarray*}\\] So the p.g.f. of \\(N^P\\) is \\[\\begin{eqnarray*} P_{N^P}(z)&amp;=&amp;\\mathrm{E}_{N^P}\\left(z^{N^P}\\right)=\\mathrm{E}_{N^L}\\left[\\mathrm{E}_{N^P}\\left(z^{N^P}|N^L\\right)\\right]\\\\ &amp;=&amp;\\mathrm{E}_{N^L}\\left[(1+v(z-1))^{N^L}\\right]\\\\ &amp;=&amp;P_{N^L}\\left(1+v(z-1)\\right) \\end{eqnarray*}\\] Thus, we can write the pgf of \\(N^P\\) as the pgf of \\(N^L\\), evaluated at a new argument \\(z^* = 1+v(z-1)\\), that is, \\(P_{N^P}(z)=P_{N^L}(z^*)\\). Special Cases: \\(N^L\\sim Poisson (\\lambda)\\). The pgf of \\(N^L\\) is \\(P_{N^L}=\\exp(\\lambda(z-1))\\). Thus the pgf of \\(N^P\\) is \\[\\begin{eqnarray*} P_{N^P}(z)&amp;=&amp;\\exp\\left( \\lambda(1+v(z-1)-1)\\right)\\\\ &amp;=&amp;\\exp(\\lambda v(z-1))\\sim Poisson (\\lambda v) \\end{eqnarray*}\\] So the payment number has the same distribution as the loss number but with the expected number of payments equal to \\(\\lambda v = \\lambda \\Pr(X&gt;d)\\). \\(N^L \\sim NegBin(\\beta, r)\\). The pgf of \\(N^L\\) is \\(P_{N^{L}}\\left( z\\right) =\\left[ 1-\\beta \\left( z-1\\right)\\right]^{-r}\\). \\[\\begin{eqnarray*} P_{N^P}(z)&amp;=&amp;\\left( 1-\\beta (1+v(z-1)-1)\\right)^{-r}\\\\ &amp;=&amp;\\left( 1-\\beta v(z-1)\\right)^{-r} \\:\\:\\:\\sim NegBin(\\beta v, r) \\end{eqnarray*}\\] So the payment number has the same distribution as the loss number but with parameters \\(\\beta v\\) and \\(r\\). Example. Suppose that loss amounts \\(X_i\\sim Pareto(\\alpha=4,\\ \\theta=150)\\). You are given that the loss frequency is \\(N^L\\sim Poisson(\\lambda)\\) and the payment frequency distribution \\(N^{P_1}\\sim Poisson (0.4)\\) with \\(d_1=30\\). Find the distribution of \\(N^{P_2}\\) with \\(d_2=100\\). Solution. Because the loss frequency \\(N^L\\) is Poisson, we can relate the means of the loss distribution \\(N^L\\) and the first payment distribution \\(N^{P_1}\\) as \\(0.4 = v_1 \\lambda\\), where \\[\\begin{align*} &amp;v_1 = \\Pr(X &gt; 30) = \\left( \\frac{150}{30+150}\\right)^4=\\left( \\frac{5}{6}\\right)^4 \\\\ \\Rightarrow \\ &amp; \\lambda = 0.4 \\left( \\frac{6}{5} \\right)^4 \\end{align*}\\] With this, we can assess the second payment distribution \\(N^{P_2}\\) as being Poisson with mean \\(\\lambda_2 = \\lambda v_2\\), where \\[\\begin{align*} &amp; v_2 = \\Pr(X&gt;100)=\\left( \\frac{150}{100+150}\\right)^4=\\left( \\frac{3}{5}\\right)^4 \\\\ \\Rightarrow \\ &amp; \\lambda_2 = \\lambda v_2 = 0.4\\left( \\frac{6}{5} \\right)^4 \\left( \\frac{3}{5} \\right)^4 = 0.1075 \\end{align*}\\] \\(\\Box\\) Follow-Up. Now suppose instead that the loss frequency is \\(N^L \\sim NegBin(\\beta,\\ r)\\) and for deductible \\(d_1=30\\), the payment frequency \\(N^{P_1}\\) is negative binomial with mean \\(0.4\\). Find the mean of the payment frequency \\(N^{P_2}\\) with deductible \\(d_2=100\\). Solution. Because the loss frequency \\(N^L\\) is negative binomial, we can relate the parameter \\(\\beta\\) of the \\(N^L\\) distribution and the parameter \\(\\beta_1\\) of the first payment distribution \\(N^{P_1}\\) using \\(\\beta_1 = \\beta v_1\\), where \\[v_1 = \\Pr(X &gt; 30) = \\left( \\frac{5}{6} \\right)^4\\] Thus, the mean of \\(N^{P_1}\\) and the mean of \\(N^L\\) are related \\[\\begin{align*} &amp;0.4 = r \\beta_1 = r \\left(\\beta v_1\\right) \\\\ \\Rightarrow \\ &amp; r\\beta = \\frac{0.4}{v_1} = 0.4 \\left(\\frac{6}{5} \\right)^4 \\end{align*}\\] Note that \\(v_2 = \\Pr(X &gt; 100) = \\left( \\frac{3}{5}\\right)^4\\) as in the original question. Then the second payment frequency distribution is \\(N^{P_2} \\sim NegBin(\\beta v_2, \\ r)\\) with mean \\[\\begin{align*} r (\\beta v_2) = (r \\beta) v_2 = 0.4 \\left( \\frac{6}{5}\\right)^4 \\left( \\frac{3}{5} \\right)^4 = 0.1075 \\end{align*}\\] \\(\\Box\\) Next we examine the more general case where \\(N^L\\) is a zero-modified distribution. Recall that a modified distribution is defined in terms of an unmodified one. That is, \\[\\begin{align*} p_k^M = c~p_k^0, {~\\rm for~} k=1,2,3,\\ldots, {~\\rm with~}c = \\frac{1-p_0^M}{1-p_0^0}. \\end{align*}\\] In the case that \\(p_0^M=0\\), we call this a ``truncated’’ distribution at zero, or \\(ZT\\). For other arbitrary values of \\(p_0^M\\), this is a zero-modified, or \\(ZM\\), distribution. The pgf for the modified distribution is shown as \\[\\begin{eqnarray*} P^M(z) = 1-c+c~P^0(z). \\end{eqnarray*}\\] When \\(N^L\\) follows a zero-modified distribution, the distribution of \\(N^P\\) is established using the same relation \\(P_{N^P}(z)=P_{N^L}\\left(1+v(z-1)\\right)\\). Special Cases: \\(N^{L}\\) is a ZM-Poisson with parameters \\(\\lambda\\) and \\(p_0^{M}\\). The pgf of \\(N^L\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}+\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}\\exp[\\lambda(z-1)].\\] Thus the pgf of \\(N^P\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}+\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}\\exp[\\lambda v(z-1)].\\] So the number of payments is also a ZM-Poisson distribution with parameters \\(\\lambda v\\) and \\(p_0^{M}\\). The probability at zero can be evaluated using \\({\\rm Pr}(N^P=0) = P_{N^P}(0)\\). \\(N^{L}\\) is a ZM-NegBin with parameters \\(\\beta\\), \\(r\\), and \\(p_0^{M}\\). The pgf of \\(N^L\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}+\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}\\left[ 1-\\beta \\left( z-1\\right)\\right]^{-r}.\\] Thus the pgf of \\(N^P\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}+\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}\\left[ 1-\\beta v\\left( z-1\\right)\\right]^{-r}.\\] So the number of payments is also a ZM-NegBin distribution with parameters \\(\\beta v\\), \\(r\\), and \\(p_0^{M}\\). Similarly, the probability at zero can be evaluated using \\({\\rm Pr}(N^P=0) = P_{N^P}(0)\\). Example. Aggregate losses are modeled as follows: (i) The number of losses follows a zero-modified Poisson distribution with \\(\\lambda=3\\) and \\(p_0^M = 0.5\\). (ii) The amount of each loss has a Burr distribution with \\(\\alpha=3, \\theta=50, \\gamma=1\\). (iii) There is a deductible of \\(d=30\\) on each loss. (iv) The number of losses and the amounts of the losses are mutually independent. Calculate \\(\\mathrm{E~} N^P\\) and \\(\\mathrm{Var~} N^P\\). Solution. Since \\(N^L\\) follows a ZM-Poisson distribution with parameters \\(\\lambda\\) and \\(p_0^M\\), we know that \\(N^P\\) also follows a ZM-Poisson distribution, but with parameters \\(\\lambda v\\) and \\(p_0^M\\), where \\[v = \\Pr(X&gt;30) = \\left( \\frac{1}{1+(30/50)} \\right)^3 = 0.2441\\] Thus, \\(N^P\\) follows a ZM-Poisson distribution with parameters \\(\\lambda^\\ast = \\lambda v= 0.7324\\) and \\(p_0^M = 0.5\\). Finally, \\[\\begin{align*} \\mathrm{E~} N^P &amp;= (1-p_0^M) \\frac{\\lambda^\\ast}{1-e^{-\\lambda^\\ast}} = 0.5 \\left( \\frac{0.7324}{1-e^{-0.7324}} \\right) \\\\ &amp;= 0.7053 \\\\ \\mathrm{Var~} N^P &amp;= (1-p_0^M) \\left( \\frac{\\lambda^\\ast[1-(\\lambda^\\ast + 1) e^{-\\lambda^\\ast}]}{(1-e^{-\\lambda^\\ast})^2} \\right) + p_0^M(1-p_0^M) \\left(\\frac{\\lambda^\\ast}{1-e^{-\\lambda^\\ast}} \\right)^2 \\\\ &amp;= 0.5 \\left( \\frac{0.7324(1-1.7324 e^{-0.7324})}{(1-e^{-0.7324})^2} \\right) + 0.5^2 \\left( \\frac{0.7324}{1-e^{-0.7324}} \\right)^2 \\\\ &amp;= 0.7244 \\end{align*}\\] \\(\\Box\\) 1.5.3 Impact of Policy Modifications on Aggregate Claims In this section, we examine how the change in deductibles affect aggregate payments from an insurance portfolio. We assume that policy limits, coinsurance, and inflation have no effect on the frequency of payments made by an insurer. As in the previous section, we further assume that deductible changes do not impact the distribution of losses for both frequency and severity. Recall the notation \\(N^L\\) for the number of losses. With ground-up loss \\(X\\) and policy deductible \\(d\\), we use \\(N^P = I(X_1&gt;d) + \\cdots + I(X_{N^L}&gt;d)\\) for the number of payments. Also, define the amount of payment on a per-loss basis as \\[\\begin{eqnarray*} X^{L}&amp;=\\left\\{ \\begin{array}{cc} 0 &amp; X&lt;\\cfrac{d}{1+r} \\\\ \\alpha[(1+r)X-d] &amp; \\cfrac{d}{1+r}\\leq X&lt;\\cfrac{u}{1+r} \\\\ \\alpha(u-d) &amp; X \\ge \\cfrac{u}{1+r}\\\\ \\end{array} \\right., \\end{eqnarray*}\\] and the the amount of payment on a per-payment basis as \\[\\begin{eqnarray*} X^{P}&amp;=\\left\\{ \\begin{array}{cc} {\\rm Undefined} &amp; X&lt;\\cfrac{d}{1+r} \\\\ \\alpha[(1+r)X-d] &amp; \\cfrac{d}{1+r}\\leq X&lt;\\cfrac{u}{1+r} \\\\ \\alpha(u-d) &amp; X \\ge \\cfrac{u}{1+r}\\\\ \\end{array} \\right.. \\end{eqnarray*}\\] In the above, \\(r\\), \\(u\\), \\(\\alpha\\) represents the inflation rate, policy limit, and coinsurance, respectively. Hence, aggregate costs (payment amounts) can be expressed either on a per loss or per payment basis: \\[\\begin{eqnarray*} S &amp;=&amp; X^L_1 + \\cdots + X^L_{N^L} \\\\ &amp;=&amp;X^P_1 + \\cdots + X^P_{N^P} . \\end{eqnarray*}\\] The fundamentals regarding collective risk models are ready to apply. For instance, we have: \\[\\begin{align*} {\\rm E}(S) &amp;= {\\rm E}\\left(N^L\\right) {\\rm E}\\left(X^L\\right) = {\\rm E}\\left(N^P\\right) {\\rm E}\\left(X^P\\right)\\\\ {\\rm Var}(S) &amp;= {\\rm E}\\left(N^L\\right) {\\rm Var}\\left(X^L\\right) + \\left[{\\rm E}\\left(X^L\\right)\\right]^2 {\\rm Var}(N^L) \\\\ &amp;= {\\rm E}\\left(N^P\\right) {\\rm Var}\\left(X^P\\right) + \\left[{\\rm E}\\left(X^P\\right)\\right]^2 {\\rm Var}(N^P)\\\\ M_S(z)&amp;=P_{N^L}\\left[M_{X^L}(z)\\right]=P_{N^P}\\left[M_{X^P}(z)\\right] \\end{align*}\\] Example. Course 3, November 2001, 6. A group dental policy has a negative binomial claim count distribution with mean 300 and variance 800. Ground-up severity is given by the following table: \\[\\begin{matrix} \\begin{array}{ c | c } \\hline \\text{Severity} &amp; \\text{Probability}\\\\ \\hline 40 &amp; 0.25\\\\ 80 &amp; 0.25\\\\ 120 &amp; 0.25\\\\ 200 &amp; 0.25\\\\ \\hline \\end{array} \\end{matrix}\\] You expect severity to increase 50% with no change in frequency. You decide to impose a per claim deductible of 100. Calculate the expected total claim payment after these changes. Solution. The cost per loss with a 50% increase in severity and a 100 deductible per claim is \\[\\begin{eqnarray*} Y^L &amp;=&amp; \\left\\{ \\begin{array}{cc} 0 &amp; 1.5x&lt;100 \\\\ 1.5x-100 &amp; 1.5x\\ge 100\\\\ \\end{array} \\right. \\end{eqnarray*}\\] This has expectation \\[\\begin{align*} \\mathrm{E~}Y^L &amp;= \\frac{1}{4} \\left[ \\left(1.5(40)-100\\right)_+ + \\left(1.5(80)-100\\right)_+ + \\left(1.5(120)-100\\right)_+ + \\left(1.5(200)-100\\right)_+ \\right] \\\\ &amp;= \\frac{1}{4}\\left[ (60-100)_+ + (120-100)_+ + (180-100)_+ + (300-100)_+\\right] \\\\ &amp;= \\frac{1}{4}\\left[ 0 + 20 + 80 + 200 \\right] = 75 \\end{align*}\\] Thus, the expected aggregate loss is \\[\\mathrm{E~}S=(\\mathrm{E~}N) \\left( \\mathrm{E~}Y^L \\right)= 300 (75) = 22,500 .\\] Follow-Up. What is \\(\\mathrm{Var~}S\\)? On a per loss basis, we have \\[\\begin{align*} \\mathrm{Var~}S &amp;= \\left(\\mathrm{E~}N \\right) \\left( \\mathrm{Var~} Y^L \\right) + \\left[ \\mathrm{E~} Y^L \\right]^2 \\left(\\mathrm{Var~} N \\right) \\end{align*}\\] where \\(\\mathrm{E~}N = 300\\) and \\(\\mathrm{Var~} N = 800\\). We find \\[\\begin{align*} &amp;\\mathrm{E} \\left[ (Y^L)^2 \\right] = \\frac{1}{4} \\left[ 0^2 + 20^2 + 80^2 + 200^2 \\right] = 11,700 \\\\ \\Rightarrow \\ &amp; \\mathrm{Var~} Y^L = \\mathrm{E} \\left[ (Y^L)^2 \\right] - \\left( \\mathrm{E~}Y^L \\right)^2 = 11,700 - 75^2 = 6,075 \\end{align*}\\] Thus, the variance of the aggregate claim payment is \\[\\begin{eqnarray*} \\mathrm{Var~}S &amp;=&amp; 300(6,075) + 75^2 (800) = 6,322,500 \\end{eqnarray*}\\] \\(\\Box\\) Alternative Method: Using the Per Payment Basis. Previously, we calculated the expected total claim payment by multiplying the expected number of losses by the expected payment per loss. Recall that we can also multiply the expected number of payments by the expected payment per payment. In this case, we have \\[S=Y_1^P + \\cdots + Y_{N_P}^P \\] The probability of a payment is \\[v=\\Pr(1.5X \\ge 100)=\\Pr(X \\ge 66.\\bar{6})=\\frac{3}{4} .\\] Thus, the number of payments, \\(N^P\\) has a negative binomial distribution with mean \\[\\mathrm{E~}N^P=300 \\left(\\frac{3}{4} \\right)=225\\] The cost per payment is \\[\\begin{eqnarray*} Y^P &amp;=&amp; \\left\\{ \\begin{array}{cc} \\text{undefined} &amp; 1.5x&lt;100 \\\\ 1.5x-100 &amp; 1.5x\\ge 100\\\\ \\end{array} \\right. \\end{eqnarray*}\\] This has expectation \\[\\mathrm{E~}Y^P=\\frac{\\mathrm{E~}Y^L}{\\Pr(1.5X &gt; 100)}= \\frac{\\mathrm{E~}Y^L}{v}=\\frac{75}{(3/4)}=100\\] Thus, as before, the expected aggregate loss is \\[\\mathrm{E~}S=\\left(\\mathrm{E~}Y^P\\right) \\left(\\mathrm{E~}N^P\\right) = 100(225)=22,500\\] \\(\\Box\\) Example. SOA Sample Question, 109. A company insures a fleet of vehicles. Aggregate losses have a compound Poisson distribution. The expected number of losses is 20. Loss amounts, regardless of vehicle type, have exponential distribution with \\(\\theta=200\\). To reduce the cost of the insurance, two modifications are to be made: (i) A certain type of vehicle will not be insured. It is estimated that this will reduce loss frequency by 20\\(\\%\\). (ii) A deductible of 100 per loss will be imposed. Calculate the expected aggregate amount paid by the insurer after the modifications. Solution. On a per loss basis, we have a 100 deductible. Thus, the expectation per loss is \\[\\begin{align*} \\mathrm{E~} Y^L &amp;= E[(X-100)_+] = E(X) - E(X\\wedge 100) \\\\ &amp;= 200 - 200(1-e^{-100/200}) = 121.31 \\end{align*}\\] Loss frequency has been reduced by 20\\(\\%\\), resulting in an expected number of losses \\[\\mathrm{E~}N^L = 0.8(20) = 16\\] Thus, the expected aggregate amount paid after the modifications is \\[\\mathrm{E~}S = \\left(\\mathrm{E~}Y^L \\right) \\left( \\mathrm{E~} N^L\\right) = 121.31(16) = 1,941\\] \\(\\Box\\) Alternative Method: Using the Per Payment Basis. We can also use the per payment basis to find the expected aggregate amount paid after the modifications. For the per payment severity, \\[\\begin{align*} \\mathrm{E~} Y^P = \\frac{\\mathrm{E~} Y^L}{\\Pr(X &gt; 100)} = \\frac{200 - 200(1-e^{-100/200})}{e^{-100/200}} = 200 \\end{align*}\\] This is not surprising – recall that the exponential distribution is memoryless, so the expected claim amounts paid in excess of 100 is still exponential with mean 200. Now we look at the payment frequency. With the deductible of 100, the probability that a payment occurs is \\(\\Pr(X &gt; 100) = e^{-100/200}\\) Thus, \\[\\mathrm{E~} N^P = 16 e^{-100/200} = 9.7\\] Putting this together, we produce the same answer using the per payment basis as the per loss basis from earlier \\[\\mathrm{E~}S = \\left( \\mathrm{E~} Y^P \\right) \\left( \\mathrm{E~} N^P \\right) = 200(9.7) = 1,941\\] \\(\\Box\\) "],
["risk-classification.html", "Chapter 2 Risk classification 2.1 Introduction 2.2 Poisson regression model 2.3 Categorical variables and multiplicative tariff 2.4 Multiplicative tariff model 2.5 Further Reading and References 2.6 Technical supplements – Estimating Poisson regression model", " Chapter 2 Risk classification Chapter Preview. This chapter motivates the use of risk classification in insurance pricing and introduces readers to the Poisson regression as a prominent example of risk classification. In Section 1 we explain why insurers need to incorporate various risk characteristics, or rating factors, of individual policyholders in pricing insurance contracts. We then introduce the Poisson regression as a pricing tool to achieve such premium differentials. The concept of exposure is also introduced in this section. As most rating factors are categorical, we show in Section 3 how the multiplicative tariff model can be incorporated in the Poisson regression model in practice, along with numerical examples for illustration. 2.1 Introduction In this section you learn: Why premiums should vary across policyholders with different risk characteristics. The meaning of the adverse selection spiral. The need of risk classification. Through insurance contracts, the policyholders effectively transfer their risks to the insurer in exchange for premiums. For the insurer to stay in business, the premium income collected from a pool of policyholders must at least equal to the benefit outgo. Ignoring the frictional expenses associated with the administrative cost and the profit margin, the net premium charged by the insurer thus should be equal to the expected loss occurring from the risk that is transferred from the policyholder. If all policyholders in the insurance pool have identical risk profiles, the insurer simply charges the same premium for all policyholders because they have the same expected loss. In reality however the policyholders are hardly homogeneous. For example, mortality risk in life insurance depends on the characteristics of the policyholder, such as, age, sex and life style. In auto insurance, those characteristics may include age, occupation, the type or use of the car, and the area where the driver resides. The knowledge of these characteristics or variables can enhance the ability of calculating fair premiums for individual policyholders as they can be used to estimate or predict the expected losses more accurately. Indeed, if the insurer do not differentiate the risk characteristics of individual policyholders and simply charges the same premium to all insureds based on the average loss in the portfolio, the insurer would face adverse selection, a situation where individuals with a higher chance of loss are attracted in the portfolio and low-risk individuals are repelled. For example, consider a health insurance industry where smoking status is an important risk factor for mortality and morbidity. Most health insurers in the market require different premiums depending on smoking status, so smokers pay higher premiums than non-smokers, with other characteristics being identical. Now suppose that there is an insurer, we will call EquitabAll, that offers the same premium to all insureds regardless of smoking status, unlike other competitors. The net premium of EquitabAll is natually an average mortality loss accounting for both smokers and non-smokers. That is, the net premium is a weighted average of the losses with the weights being the proportion of smokers and non-smokers, respectively. Thus it is easy to see that that a smoker would have a good incentive to purchase insurance from EquitabAll than from other insurers as the offered premium by EquitabAll is relatively lower. At the same time non-smokers would prefer buying insurance from somewhere else where lower premiums, computed from the non-smoker group only, are offered. As a result, there will be more smokers and less non-smokers in the EquitabAll’s portfolio, which leads to larger-than-expected losses and hence a higher premium for insureds in the next period to cover the higher costs. With the raised new premium in the next period, non-smokers in EquitabAll will have even greater incentives to switch the insurer. As this cycle continues over time, EquitabAll would gradually retain more smokers and less non-smokers in its portfolio with the premium continually raised, eventually leading to a collapsing of business. In the literature this phenomenon is known as the adverse selection spiral or death spiral. Therefore, incorporating and differentiating important risk characteristics of individuals in the insurance pricing process are a pertinent component for both the determination of fair premium for individual policyholders and the long term sustainability of insurers. In order to incorporate relevant risk characteristics of policyholders in the pricing process, insurers maintain some classification system that assigns each policyholder to one of the risk classes based on a relatively small number of risk characteristics that are deemed most relevant. These characteristics used in the classification system are called the rating factors, which are a priori variables in the sense that they are known before the contract begins (e.g., sex, health status, vehicle type, etc, are known during the underwriting). All policyholders sharing identical risk factors thus are assigned to the same risk class, and are considered homogeneous from the pricing viewpoint; the insurer consequently charge them the same premium. An important task in any risk classification is to construct a quantitative model that can determine the expected loss given various rating factors of a policyholder. The standard approach is to adopt a statistical regression model which produces the expected loss as the output when the relevant risk factors are given as the inputs. In this chapter we learn the Poisson regression, which can be used when the loss is a count variable, as a prominent example of an insurance pricing tool. 2.2 Poisson regression model The Poisson regression model has been successfully used in a wide range of applications and has an advantage of allowing closed-form expressions for important quantities, which provides a informative intuition and interpretation. In this section we introduce the Poisson regression as a natural extension of the Poisson distribution. In this section you will: Understand Poisson regressions as convenient tool to combine individual Poisson distributions in a unified fashion. Learn the concept of exposure and its importance. Formally learn how to formulate the Poisson regression model using indicator variables when the explanatory variables are categorical. 2.2.1 Need of Poisson regression Poisson distribution To introduce the Poisson regression, let us consider a hypothetical health insurance portfolio where all policyholders are of the same age and only one risk factor, smoking status, is relevant. Smoking status thus is a categorical variable containing two different types: smoker and non-smoker. In the statistical literature different types in a given categorical variable are commonly called levels. As there are two levels for the smoking status, we may denote smoker and non-smoker by level 1 and 2, respectively. Here the numbering is arbitrary and nominal. Suppose now that we are interested in pricing a health insurance where the premium for each policyholder is determined by the number of outpatient visits to doctor’s office during a year. The amount of medical cost for each visit is assumed to be the same regardless of the smoking status for simplicity. Thus if we believe that smoking status is a valid risk factor in this health insurance, it is natural to consider the data separately for each smoking status. In Table 8.1 we present the data for this portfolio. \\[\\begin{matrix} \\begin{array}{cc|cc|cc} \\hline \\text{Smoker} &amp; \\text{(level 1)} &amp; \\text{Non-smoker}&amp;\\text{(level 2)} &amp; &amp; \\text{Both}\\\\ \\text{Count} &amp; \\text{Observed} &amp; \\text{Count} &amp; \\text{Observed} &amp; \\text{Count} &amp; \\text{Observed} \\\\ \\hline 0 &amp; 2213 &amp; 0 &amp; 6671 &amp; 0 &amp; 8884 \\\\ 1 &amp; 178 &amp; 1 &amp; 430 &amp; 1 &amp; 608 \\\\ 2 &amp; 11 &amp; 2 &amp; 25 &amp; 2 &amp; 36 \\\\ 3 &amp; 6 &amp; 3 &amp; 9 &amp; 3 &amp; 15 \\\\ 4 &amp; 0 &amp; 4 &amp; 4 &amp; 4 &amp; 4 \\\\ 5 &amp; 1 &amp; 5 &amp; 2 &amp; 5 &amp; 3 \\\\ \\hline \\text{Total} &amp; 2409 &amp; \\text{Total} &amp; 7141 &amp; \\text{Total} &amp; 9550 \\\\ \\text{Mean} &amp; 0.0926 &amp; \\text{Mean} &amp; 0.0746 &amp; \\text{Mean} &amp; 0.0792 \\\\ \\hline \\end{array} \\end{matrix}\\] [Table 8.1] : Number of visits to doctor’s office in last year As this dataset contains random counts we try to fit a Poisson distribution for each level. The pmf of the Poisson with mean \\(\\mu\\) is given by \\[\\begin{equation} \\Pr(Y=y)=\\frac{\\mu^y e^{-\\mu}}{y!},\\qquad y=0,1,2, \\ldots - (1) \\end{equation}\\] and \\(\\mathrm{E~}{(Y)}=\\mathrm{Var~}{(Y)}=\\mu\\). Furthermore, the mle of the Poisson distribution is given by the sample mean. Thus if we denote the Poisson mean parameter for each level by \\(\\mu_{(1)}\\) (smoker) and \\(\\mu_{(2)}\\) (non-smoker), we see from Table 8.1 that \\(\\hat{\\mu}_{(1)}=0.0926\\) and \\(\\hat{\\mu}_{(2)}=0.0746\\). This simple example shows the basic idea of risk classification. Depending on the smoking status a policyholder will have a different risk characteristic and it can be incorporated through varying Poisson parameter in computing the fair premium. In this example the ratio of expected loss frequencies is \\(\\hat{\\mu}_{(1)}/\\hat{\\mu}_{(2)}=1.2402\\), implying that smokers tend to visit doctor’s office 24.02\\(\\%\\) times more frequently compared to non-smokers. It is also informative to note that if the insurer charges the same premium to all policyholders regardless of the smoking status, based on the average characteristic of the portfolio, as was the case for EquitabAll described in Introduction, the expected frequency (or the premium) \\(\\hat{\\mu}\\) is 0.0792, obtained from the last column of Table 8.1. It is easily verified that \\[\\begin{equation} \\hat{\\mu} = \\left(\\frac{n_1}{n_1+n_2}\\right)\\hat{\\mu}_{(1)}+\\left(\\frac{n_2}{n_1+n_2}\\right)\\hat{\\mu}_{(2)}=0.0792, - (2) \\end{equation}\\] where \\(n_i\\) is the number of observations in each level. Clearly, this premium is a weighted average of the premiums for each level with the weight equal to the proportion of the insureds in that level. A simple Poisson regression In the example above, we have fitted a Poisson distribution for each level separately, but we can actually combine them together in a unified fashion so that a single Poisson model can encompass both smoking and non-smoking statuses. This can be done by relating the Poisson mean parameter with the risk factor. In other words, we make the Poisson mean, which is the expected loss frequency, respond to the change in the smoking status. The conventional approach to deal with a categorical variable is to adopt indicator or dummy variables that take either 1 or 0, so that we turn the switch on for one level and off for others. Therefore we may propose to use \\[\\begin{equation} \\mu=\\beta_0+\\beta_1 x_1 - (3) \\end{equation}\\] or, more commonly, a log linear form \\[\\begin{equation} \\log \\mu=\\beta_0+\\beta_1 x_1, - (4) \\end{equation}\\] where \\(x_1\\) is an indicator variable with \\[\\begin{equation} x_1= \\begin{cases} 1 &amp; \\text{if smoker}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} - (5) \\end{equation}\\] We generally prefer the log linear relation (4) to the linear one in (3) to prevent undesirable events of producing negative \\(\\mu\\) values, which may happen when there are many different risk factors and levels. The setup (4) and (5) then results in different Poisson frequency parameters depending on the level in the risk factor: \\[\\begin{equation} \\log \\mu= \\begin{cases} \\beta_0+\\beta_1 \\\\ \\beta_0 \\end{cases} \\quad \\text{or equivalently,}\\qquad \\mu= \\begin{cases} e^{\\beta_0+\\beta_1} &amp; \\text{if smoker (level 1)}, \\\\ e^{\\beta_0} &amp; \\text{if non-smoker (level 2)}, \\end{cases} - (6) \\end{equation}\\] achieving what we aim for. This is the simplest form of the Poisson regression. Note that we require a single indicator variable to model two levels in this case. Alternatively, it is also possible to use two indicator variables through a different coding scheme. This scheme requires dropping the intercept term so that (4) is modified to \\[\\begin{equation} \\log \\mu=\\beta_1 x_1+\\beta_2 x_2, - (7) \\end{equation}\\] where \\(x_2\\) is the second indicator variable with \\[\\begin{equation} x_2= \\begin{cases} 1 &amp; \\text{if non-smoker}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} - (8) \\end{equation}\\] Then we have, from (7), \\[\\begin{equation} \\log \\mu= \\begin{cases} \\beta_1 \\\\ \\beta_2 \\end{cases} \\quad \\text{or}\\qquad \\mu= \\begin{cases} e^{\\beta_1} &amp; \\text{if smoker (level 1)}, \\\\ e^{\\beta_2} &amp; \\text{if non-smoker (level 2)}. \\end{cases} - (9) \\end{equation}\\] The numerical result of (6) is the same as (9) as all the coefficients are given as numbers in actual estimation, with the former setup more common in most texts; we also stick to the former. With this Poisson regression model we can easily understand how the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are linked to the expected loss frequency in each level. According to (6), the Poisson mean of the smokers, \\(\\mu_{(1)}\\), is given by \\[\\begin{equation} \\mu_{(1)}=e^{\\beta_0+\\beta_1}=\\mu_{(2)} \\,e^{\\beta_1} \\quad \\text{or}\\quad \\mu_{(1)}/\\mu_{(2)} =e^{\\beta_1} - (10) \\end{equation}\\] where \\(\\mu_{(2)}\\) is the Poisson mean for the non-smokers. This relation between the smokers and non-smokers suggests a useful way to compare the risks embedded in different levels of a given risk factor. That is, the proportional increase in the expected loss frequency of the smokers compared to that of the non-smokers is simply given by a multiplicative factor \\(e^{\\beta_1}\\). Putting another way, if we set the expected loss frequency of the non-smokers as the base value, the expected loss frequency of the smokers is obtained by applying \\(e^{\\beta_1}\\) to the base value. Dealing with multi-level case We can readily extend the two-level case to a multi-level one where \\(l\\) different levels are involved for a single rating factor. For this we generally need \\(l-1\\) indicator variables to formulate \\[\\begin{equation} \\log \\mu=\\beta_0+\\beta_1 x_1+\\ldots+\\beta_{l-1} x_{l-1}, - (11) \\end{equation}\\] where \\(x_k\\) is an indicator variable that takes 1 if the policy belongs to level \\(k\\) and 0 otherwise, for \\(k=1,2, \\ldots, l-1\\). By omitting the indicator variable associated with the last level in (11) we effectively chose level \\(l\\) as the base case, but this choice is arbitrary and does not matter numerically. The resulting Poisson parameter for policies in level \\(k\\) then becomes, from (11), \\[\\begin{equation} \\nonumber \\mu= \\begin{cases} e^{\\beta_0+\\beta_k} &amp; \\text{if the policy belongs to level k (k=1,2, ..., l-1)}, \\\\ e^{\\beta_0} &amp; \\text{if the policy belongs to level l}. \\end{cases} \\end{equation}\\] Thus if we denote the Poisson parameter for policies in level \\(k\\) by \\(\\mu_{(k)}\\), we can relate the Poisson parameter for different levels through \\(\\mu_{(k)}=\\mu_{(l)}\\, e^{\\beta_k}\\), \\(k=1,2, \\ldots, l-1\\). This indicates that, just like the two-level case, the expected loss frequency of the \\(k\\)th level is obtained from the base value multiplied by the relative factor \\(e^{\\beta_k}\\). This relative interpretation becomes more powerful when there are many risk factors with multi-levels, and leads us to a better understanding of the underlying risk and more accurate prediction of future losses. Finally, we note that the varying Poisson mean is completely driven by the coefficient parameters \\(\\beta_k\\)’s, which are to be estimated from the dataset; the procedure of the parameter estimation will be discussed later in this chapter. 2.2.2 Poisson regression We now describe the Poisson regression in a formal and more general setting. Let us assume that there are \\(n\\) independent policyholders with a set of rating factors characterized by a \\(k\\)-variate vector1. The \\(i\\)th policyholder’s rating factor is thus denoted by vector \\(\\mathbf{ x}_i=(1, x_{i1}, \\ldots, x_{ik})^{\\prime}\\), and the policyholder has recorded the loss count \\(y_i \\in \\{0,1,2, \\ldots \\}\\) from the last period of loss observation, for \\(i=1, \\ldots, n\\). In the regression literature, the values \\(x_{i1}, \\ldots, x_{ik}\\) are generally known as the explanatory variables, as these are measurements providing information about the variable of interest \\(y_i\\). In essence, regression analysis is a method to quantify the relationship between a variable of interest and explanatory variables. We also assume, for now, that all policyholders have the same one unit period for loss observation, or equal exposure of 1, to keep things simple; we will discuss more details on the exposure in the following subsection. As done before, we describe the Poisson regression through its mean function. For this we first denote \\(\\mu_i\\) to be the expected loss count of the \\(i\\)th policyholder under the Poisson specification (1): \\[\\begin{equation} \\mu_i=\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n. - (12) \\end{equation}\\] The condition inside the expectation operation in (12) indicates that the loss frequency \\(\\mu_i\\) is the model output responding to the given set of risk factors or explanatory variables. In principle the conditional mean \\(\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}\\) in (12) can take different forms depending on how we specify the relationship between \\(\\mathbf{ x}\\) and \\(y\\). The standard choice for the Poisson regression is to adopt the exponential function, as we mentioned previously, so that \\[\\begin{equation} \\mu_i=\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}=e^{\\mathbf{ x}^{\\prime}_i\\beta}, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n. - (13) \\end{equation}\\] Here \\(\\beta=(\\beta_0, \\ldots, \\beta_k)^{\\prime}\\) is the vector of coefficients so that \\(\\mathbf{ x}^{\\prime}_i\\beta=\\beta_0+\\beta_1x_{i1} +\\ldots+\\beta_k x_{ik}\\). The exponential function in (13) ensures that \\(\\mu_i &gt;0\\) for any set of rating factors \\(\\mathbf{ x}_i\\). Often (13) is rewritten as a log linear form \\[\\begin{equation} \\log \\mu_i=\\log \\mathrm{E~}{(y_i|\\mathbf{ x}_i)}=\\mathbf{ x}^{\\prime}_i\\beta, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n - (14) \\end{equation}\\] to reveal the relationship when the right side is set as the linear form, \\(\\mathbf{ x}^{\\prime}_i\\beta\\). Again, we see that the mapping works well as both sides of (14), \\(\\log \\mu_i\\) and \\(\\mathbf{ x}_i\\beta\\), can now cover the entire real values. This is the formulation of the Poisson regression, assuming that all policyholders have the same unit period of exposure. When the exposures differ among the policyholders, however, as is the case in most practical cases, we need to revise this formulation by adding exposure component as an additional term in (14). 2.2.3 Incorporating exposure Concept of exposure In order to determine the size of potential losses in any type of insurance, one must always know the corresponding exposure. The concept of exposure is an extremely important ingredient in insurance pricing, though we usually take it for granted. For example, when we say the expected claim frequency of a health insurance policy is 0.2, it does not mean much without the specification of the exposure such as, in this case, per month or per year. In fact, all premiums and losses need the exposure precisely specified and must be quoted accordingly; otherwise all subsequent statistical analyses and predictions will be distorted. In the previous section we assumed the same unit of exposure across all policyholders, but this is hardly realistic in practice. In health insurance, for example, two different policyholders with different lengths of insurance coverage (e.g., 3 months and 12 months, respectively) could have recorded the same number of claim counts. As the expected number of claim counts would be proportional to the length of coverage, we should not treat these two policyholders’ loss experiences identically in the modelling process. This motivates the need of the concept of exposure in the Poisson regression. The Poisson distribution in (1) is parametrised via its mean. To understand the exposure, we alternatively parametrize the Poisson pmf in terms of the rate parameter \\(\\lambda\\), based on the definition of the Poisson process: \\[\\begin{equation} \\Pr(Y=y)=\\frac{(\\lambda t)^y e^{-\\lambda t}}{y!},\\qquad y=0,1,2, \\ldots - (15) \\end{equation}\\] with \\(\\mathrm{E~}{(Y)}=\\mathrm{Var~}{(Y)}=\\lambda t\\). Here \\(\\lambda\\) is known as the rate or intensity per unit period of the Poisson process and \\(t\\) represents the length of time or exposure, a known constant value. For given \\(\\lambda\\) the Poisson distribution (15) produces a larger expected loss count as the exposure \\(t\\) gets larger. Clearly, (15) reduces to (1) when \\(t=1\\), which means that the mean and the rate become the same for the unit exposure, the case we considered in the previous subsection. In principle the exposure does not need to be measured in units of time and may represent different things depending the problem at hand. For example, In health insurance, the rate may be the occurrence of a specific disease per 1,000 people and the exposure is the number of people considered in the unit of 1,000. In auto insurance, the rate may be the number of accidents per year of a driver and the exposure is the length of the observed period for the driver in the unit of year. For workers compensation, the rate may be the probability of injury in the course of employment per dollar and the exposure is the payroll amount in dollar. In marketing, the rate may be the number of customers who enter a store per hour and the exposure is the number of hours observed. In civil engineering, the rate may be the number of major cracks on the paved road per 10 kms and the exposure is the length of road considered in the unit of 10 kms. In credit risk modelling, the rate may be the number of default events per 1000 firms and the exposure is the number of firms under consideration in the unit of 1,000. Actuaries may be able to use different exposure bases for a given insurable loss. For example, in auto insurance, both the number of kilometres driven and the number of months coved by insurance can be used as exposure bases. Here the former is more accurate and useful in modelling the losses from car accidents, but more difficult to measure and manage for insurers. Thus, a good exposure base may not be the theoretically best one due to various practical constraints. As a rule, an exposure base must be easy to determine, accurately measurable, legally and socially acceptable, and free from potential manipulation by policyholders. Incorporating exposure in Poisson regression As exposures affect the Poisson mean, constructing Poisson regressions requires us to carefully separate the rate and exposure in the modelling process. Focusing on the insurance context, let us denote the rate of the loss event of the \\(i\\)th policyholder by \\(\\lambda_i\\), the known exposure (the length of coverage) by \\(m_i\\) and the expected loss count under the given exposure by \\(\\mu_i\\). Then the Poisson regression formulation in (13) and (14) should be revised in light of (15) as \\[\\begin{equation} \\mu_i=\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}=m_i \\,\\lambda_i=m_i \\, e^{\\mathbf{ x}^{\\prime}_i\\beta}, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n, - (16) \\end{equation}\\] which gives \\[\\begin{equation} \\log \\mu_i=\\log m_i+\\mathbf{ x}^{\\prime}_i\\beta, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, - (17) \\end{equation}\\] Adding \\(\\log m_i\\) in (17) does not pose a problem in fitting as we can always specify this as an extra explanatory variable, as it is a known constant, and fix its coefficient to 1. In the literature the log of exposure, \\(\\log m_i\\), is commonly called the offset. 2.2.4 Exercises Regarding Table 8.1 answer the followings. Verify the mean values in the table. Verify the number in Equation (2). Produce the fitted Poisson counts for each smoking status in the table. In the Poisson regression formulation (12), consider using \\(\\mu_i=\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}=({\\mathbf{ x}^{\\prime}_i\\beta})^2\\), for \\(i=1, \\ldots, n\\), instead of the exponential function What potential issue would you have? Verify Equation (26) by differentiating the log-likelihood (23). 2.3 Categorical variables and multiplicative tariff In this section you will learn: The multiplicative tariff model when the rating factors are categorical. How to construct the Poisson regression model based on the multiplicative tariff structure. 2.3.1 Rating factors and tariff In practice most rating factors in insurance are categorical variables, meaning that they take one of the pre-determined number of possible values. Examples of categorical variables include sex, type of cars, the driver’s region of residence and occupation. Continuous variables, such as age or auto mileage, can also be grouped by bands and treated as categorical variables. Thus we can imagine that, with a small number of rating factors, there will be many policyholders falling into the same risk class, charged with the same premium. For the remaining of this chapter we assume that all rating factors are categorical variables. To illustrate how categorical variables are used in the pricing process, we consider a hypothetical auto insurance with only two rating factors: Type of vehicle: Type A (personally owned) and B (owned by corporations). We use index \\(j=1\\) and \\(2\\) to respectively represent each level of this rating factor. Age band of the driver: Young (age \\(&lt;\\) 25), middle (25 \\(\\le\\) age \\(&lt;\\) 60) and old age (age \\(\\ge\\) 60). We use index \\(k=1, 2\\) and \\(3\\), respectively, for this rating factor. From this classification rule, we may create an organized table or list, such as the one shown in Table 8.2, collected from all policyholders. Clearly there are \\(2 \\times 3=6\\) different risk classes in total. Each row of the table shows a combination of different risk characteristics of individual policyholders. Our goal is to compute six different premiums for each of these combinations. Once the premium for each row has been determined using the given exposure and claim counts, the insurer can replace the last two columns in Table 8.2 with a single column containing the computed premiums. This new table then can serve as a manual to determine the premium for a new policyholder given the rating factors during the underwriting process. In non-life insurance, a table (or a set of tables) or list that contains each set of rating factors and the associated premium is referred to as a tariff. Each unique combination of the rating factors in a tariff is called a tariff cell; thus, in Table 8.2 the number of tariff cells is six, same as the number of risk classes. \\[\\begin{matrix} \\begin{array}{ccrrc} \\hline \\text{Rating} &amp;\\text{factors} &amp; \\text{Exposure} &amp; \\text{Claim count} \\\\ \\text{Type }(j) &amp; \\text{Age }(k) &amp; \\text{in year} &amp; \\text{observed}\\\\ \\hline \\hline j=1 &amp; k=1 &amp; 89.1 &amp; 9\\\\ 1 &amp; 2 &amp; 208.5&amp; 8\\\\ 1 &amp; 3 &amp; 155.2 &amp; 6 \\\\ 2 &amp; 1 &amp; 19.3 &amp; 1 \\\\ 2 &amp; 2 &amp; 360.4 &amp; 13 \\\\ 2 &amp; 3 &amp; 276.7 &amp; 6 \\\\ \\hline \\end{array} \\end{matrix}\\] [Table 8.2] : Loss record of the illustrative auto insurer Let us now look at the loss information in Table 8.2 more closely. The exposure in each row represents the sum of the length of insurance coverages, or in-force times, in the unit of year, of all the policyholders in that tariff cell. Similarly the claim counts in each row is the number of claims at each cell. Naturally the exposures and claim counts vary due to the different number of drivers across the cells, as well as different in-force time periods among the drivers within each cell. In light of the Poisson regression framework, we denote the exposure and claim count of cell \\((j,k)\\) as \\(m_{jk}\\) and \\(y_{jk}\\), respectively, and define the claim count per unit exposure as \\[\\begin{equation} \\label{z.jk}\\nonumber z_{jk}= \\frac{y_{jk}}{ m_{jk}}, \\qquad j=1,2;\\, k=1, 2,3. \\end{equation}\\] For example, \\(z_{12}=8/208.5=0.03837\\), meaning that a policyholder in tariff cell (1,2) would have 0.03837 accidents if insured for a full year on average. The set of \\(z_{ij}\\) values then corresponds to the rate parameter in the Poisson distribution (15) as they are the event occurrence rates per unit exposure. That is, we have \\(z_{jk}=\\hat{\\lambda}_{jk}\\) where \\({\\lambda}_{jk}\\) is the Poisson rate parameter. Producing \\(z_{ij}\\) values however does not do much beyond comparing the average loss frequencies across risk classes. To fully exploit the dataset, we will construct a pricing model from Table 8.2 using the Poisson regression, for the remaining part of the chapter. We comment that actual loss records used by insurers typically include much more risk factors, in which case the number of cells grows exponentially. The tariff would then consist of a set of tables, instead of one, separated by some of the basic rating factors, such as sex or territory. 2.4 Multiplicative tariff model In this subsection, we introduce the multiplicative tariff model, a popular pricing structure that can be naturally used within the Poisson regression framework. The developments here is based on Table 8.2. Recall that the loss count of a policyholder is described by the Poisson regression model with rate \\(\\lambda\\) and the exposure \\(m\\), so that the expected loss count becomes \\(m\\lambda\\). As \\(m\\) is a known constant, we are essentially concerned with modelling \\(\\lambda\\), so that it responds to the change in the rating factors. Among other possible functional forms, we commonly choose the multiplicative2 relation to model the Poisson rate \\(\\lambda_{jk}\\) for rating factor (\\(j,k\\)): \\[\\begin{equation} \\lambda_{jk}= f_0 \\times f_{1j} \\times f_{2k}, \\qquad j=1,2;\\, k=1, 2,3. - (18) \\end{equation}\\] Here \\(\\{ f_{1j}, j=1,2\\}\\) are the parameters associated with the two levels in the first rating factor, car type, and \\(\\{ f_{2k}, k=1,2,3\\}\\) associated with the three levels in the age band, the second rating factor. For instance, the Poisson rate for a mid-aged policyholder with a Type B vehicle is given by \\(\\lambda_{22}=f_0 \\times f_{12} \\times f_{22}\\). The first term \\(f_0\\) is some base value to be discussed shortly. Thus these six parameters are understood as numerical representations of the levels within each rating factor, and are to be estimated from the dataset. The multiplicative form (18) is easy to understand and use, because it clearly shows how the expected loss count (per unit exposure) changes as each rating factor varies. For example, if \\(f_{11}=1\\) and \\(f_{12}=1.2\\), then the expected loss count of a policyholder with a vehicle of type B would be 20\\(\\%\\) larger than type A, when the other factors are the same. In non-life insurance, the parameters \\(f_{1j}\\) and \\(f_{2k}\\) are known as relativities as they determine how much expected loss should change relative to the base value \\(f_0\\). The idea of relativity is quite convenient in practice, as we can decide the premium for a policyholder by simply multiplying a series of corresponding relativities to the base value. Dropping an existing rating factor or adding a new one is also transparent with this multiplicative structure. In addition, the insurer may easily adjust the overall premium for all policyholders by controlling the base value \\(f_0\\) without changing individual relativities. However, by adopting the multiplicative form, we implicitly assume that there is no serious interaction among the risk factors. When the multiplicative form is used we need to address an identification issue. That is, for any \\(c&gt;0\\), we can write \\[\\begin{equation} \\lambda_{jk}= f_0 \\times \\frac{f_{1j}}{c} \\times c\\,f_{2k}. \\end{equation}\\] By comparing with (18), we see that the identical rate parameter \\(\\lambda_{jk}\\) can be obtained for very different individual relativities. This over-parametrization, meaning that many different sets of parameters arrive at the identical model, obviously calls for some restriction on \\(f_{1j}\\) and \\(f_{2k}\\). The standard practice is to make one relativity in each rating factor equal to one. This can be made arbitrarily, so we will assume that \\(f_{11}=1\\) and \\(f_{21}=1\\) for our purpose. This way all other relativities are uniquely determined. The tariff cell \\((j,k)=(1,1)\\) is then called the base tariff cell, where the rate simply becomes \\(\\lambda_{11}=f_0\\), corresponding to the base value according to (18). Thus the base value \\(f_0\\) is generally interpreted as the Poisson rate of the base tariff cell. Again, (18) is log-transformed and rewritten as \\[\\begin{equation} \\log \\lambda_{jk}= \\log f_0 + \\log f_{1j} + \\log f_{2k}, - (19) \\end{equation}\\] as it is easier to work with in estimating process, similar to (14). This log linear form makes the log relativities of the base level in each rating factor equal to zero, i.e., \\(\\log f_{11}=\\log f_{21}=0\\), and leads to the following alternative, more explicit expression for (19): \\[\\begin{equation} \\log \\lambda=\\begin{cases} \\log f_0 + \\quad 0 \\quad \\,\\,+ \\quad 0 \\quad \\,\\,&amp; \\text{for a policy in cell $(1,1)$}, \\\\ \\log f_0+ \\quad 0 \\quad \\,\\,+\\log f_{22}&amp; \\text{for a policy in cell $(1,2)$}, \\\\ \\log f_0+ \\quad 0 \\quad \\,\\,+\\log f_{23}&amp; \\text{for a policy in cell $(1,3)$}, \\\\ \\log f_0+\\log f_{12}+ \\quad 0 \\quad \\,\\,&amp; \\text{for a policy in cell $(2,1)$}, \\\\ \\log f_0+\\log f_{12}+\\log f_{22}&amp; \\text{for a policy in cell $(2,2)$}, \\\\ \\log f_0+\\log f_{12}+\\log f_{23}&amp; \\text{for a policy in cell $(2,3)$}. \\\\ \\end{cases} - (20) \\end{equation}\\] This clearly shows that the Poisson rate parameter \\(\\lambda\\) varies across different tariff cells, with the same log linear form used in the Poisson regression framework. In fact the reader may see that (20) is an extended version of the early expression (6) with multiple risk factors and that the log relativities now play the role of \\(\\beta_i\\) parameters. Therefore all the relativities can be readily estimated via fitting a Poisson regression with a suitably chosen set of indicator variables. 2.4.1 Poisson regression for multiplicative tariff Indicator variables for tariff cells We now explain how the relativities can be incorporated in the Poisson regression. As seen early in this chapter we use indicator variables to deal with categorial variables. For our illustrative auto insurer, therefore, we define an indicator variable for the first rating factor as \\[\\begin{equation} x_1= \\begin{cases} 1 &amp; \\text{ for vehicle type B}, \\\\ 0 &amp; \\text{ otherwise}. \\end{cases} \\end{equation}\\] For the second rating factor, we employ two indicator variables for the age band, that is, \\[\\begin{equation} x_2= \\begin{cases} 1 &amp; \\text{for age band 2}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation}\\] and \\[\\begin{equation} x_3= \\begin{cases} 1 &amp; \\text{for age band 3}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation}\\] The triple \\((x_1, x_2, x_3)\\) then can effectively and uniquely determine each risk class. By observing that the indicator variables associated with Type A and Age band 1 are omitted, we see that tariff cell \\((j,k)=(1,1)\\) plays the role of the base cell. We emphasize that our choice of the three indicator variables above has been carefully made so that it is consistent with the choice of the base levels in the multiplicative tariff model in the previous subsection (i.e., \\(f_{11}=1\\) and \\(f_{21}=1\\)). With the proposed indicator variables we can rewrite the log rate (19) as \\[\\begin{equation} \\log \\lambda_{}= \\log f_0+ \\log f_{12} \\times x_1 + \\log f_{22} \\times x_2 +\\log f_{23} \\times x_3, - (21) \\end{equation} \\] which is identical to (20) when each triple value is actually applied. For example, we can verify that the base tariff cell \\((j,k)=(1,1)\\) corresponds to \\((x_1, x_2,x_3)=(0, 0, 0)\\), and in turn produces \\(\\log \\lambda=\\log f_0\\) or \\(\\lambda= f_0\\) in (21) as required. Poisson regression for the tariff model} Under this specification, let us consider \\(n\\) policyholders in the portfolio with the \\(i\\)th policyholder’s risk characteristic given by a vector of explanatory variables \\(\\mathbf{ x}_i=(x_{i1}, x_{i2},x_{i3})^{\\prime}\\), for \\(i=1, \\ldots, n\\). We then recognize (21) as \\[\\begin{equation} \\log \\lambda_{i}= \\beta_0+ \\beta_1 \\, x_{i1} + \\beta_{2} \\, x_{i2} +\\beta_3 \\, x_{i3}=\\mathbf{ x}^{\\prime}_i\\beta, \\qquad i=1, \\ldots, n, \\end{equation}\\] where \\(\\beta_0, \\ldots, \\beta_3\\) can be mapped to the corresponding log relativities in (21). This is exactly the same setup as in (17) except for the exposure component. Therefore, by incorporating the exposure in each risk class, the Poisson regression model for this multiplicative tariff model finally becomes \\[\\begin{equation} \\log \\mu_i=\\log \\lambda_{i}+\\log m_i= \\log m_i+ \\beta_0+ \\beta_1 \\, x_{i1} + \\beta_{2} \\, x_{i2} +\\beta_3 \\, x_{i3}=\\log m_i+\\mathbf{ x}^{\\prime}_i\\beta, \\end{equation}\\] for \\(i=1, \\ldots, n\\). As a result, the relativities are given by \\[\\begin{equation} {f}_0=e^{\\beta_0}, \\quad {f}_{12}=e^{\\beta_1}, \\quad {f}_{22}=e^{\\beta_2} \\quad \\text{and}\\quad {f}_{23}=e^{\\beta_3}, - (22) \\end{equation}\\] with \\(f_{11}=1\\) and \\(f_{21}=1\\) from the original construction. For the actual dataset, \\(\\beta_i\\), \\(i=0,1, 2, 3\\), is replaced with the mle \\(b_i\\) using the method in the technical supplement at the end of this chapter (Section 1.6). 2.4.2 Numerical examples We present two numerical examples of the Poisson regression. In the first example we construct a Poisson regression model from Table 8.2, which is a dataset of a hypothetical auto insurer. The second example uses an actual industry dataset with more risk factors. As our purpose is to show how the Poisson regression model can be used under a given classification rule, we are not concerned with the quality of the Poisson model fit in this chapter. Example 1: Poisson regression for the illustrative auto insurer In the last few subsections we considered a dataset of a hypothetical auto insurer with two risk factors, as given in Table 8.2. We now apply the Poisson regression model to this dataset. As done before, we have set \\((j,k)=(1,1)\\) as the base tariff cell, so that \\(f_{11}=f_{21}=1\\). The result of the regression gives the coefficient estimates \\((b_0, b_1,b_2,b_3)=(-2.3359, -0.3004, -0.7837, -1.0655 )\\), which in turn produces the corresponding relativities \\[\\begin{equation} \\label{relativity.eg1} \\nonumber {f}_0=0.0967, \\quad {f}_{12}= 0.7405, \\quad {f}_{22}=0.4567 \\quad \\text{and}\\quad {f}_{23}=0.3445. \\end{equation}\\] from the relation given in (22). The R script and the output are given below: &gt; mydat1&lt;- read.csv(&quot;eg1_v1a.csv&quot;) &gt; mydat1 Vtype Agebnd Expsr Claims 1 1 1 89.1 9 2 1 2 208.5 8 3 1 3 155.2 6 4 2 1 19.3 1 5 2 2 360.4 13 6 2 3 276.7 6 &gt; VtypeF &lt;- relevel(factor(Vtype), ref=&quot;1&quot;) # treat Vtype as factors with 1 as base. &gt; AgebndF &lt;- relevel(factor(Agebnd), ref=&quot;1&quot;) # treat Age band as factors. &gt; Pois_reg1 = glm(Claims ~ VtypeF + AgebndF, data = mydat1, family = poisson(link = log), offset = log(Expsr) ) &gt; Pois_reg1 Coefficients: (Intercept) VtypeF2 AgebndF2 AgebndF3 -2.3359 -0.3004 -0.7837 -1.0655 Degrees of Freedom: 5 Total (i.e. Null); 2 Residual Null Deviance: 8.774 Residual Deviance: 0.6514 AIC: 30.37 Example 2: Poisson regression for Singapore insurance claims data This actual data is a subset of the data used by (???). The data is from the General Insurance Association of Singapore, an organisation consisting of non-life insurers in Singapore. The data contains the number of car accidents for \\(n=7,483\\) auto insurance policies with several categorical explanatory variables and the exposure for each policy. The explanatory variables include four risk factors: the type of the vehicle insured (either automobile (A) or other (O), denoted by \\(\\verb&quot;Vtype&quot;\\)), the age of the vehicle in years (\\(\\verb&quot;Vage&quot;\\)), gender of the policyholder (\\(\\verb&quot;Sex&quot;\\)) and the age of the policyholder (in years, grouped into seven categories, denoted \\(\\verb&quot;Age&quot;\\)). Based on the data description, there are several things to remember before constructing a model (May need the table from the Jed’s pdf file). First, there are 3,842 policies with vehicle type A (automobile) and 3,641 policies with other vehicle types. However, age and sex information is available for the policies of vehicle type A only; the drivers of all other types of vehicles are recorded to be aged 21 or less with sex unspecified, except for one policy, indicating that no driver information has been collected for non-automobile vehicles. Second, type A vehicles are all classified as private vehicles and all the other types are not. When we include these risk factors, we assume that all unspecified sex to be male. As the age information is only applicable to type A vehicles, we set the model accordingly. That is, we apply the age variable only to vehicles of type A. Also we used five vehicle age bands, simplifying the original seven bands, by combining vehicle ages 0,1 and 2; the combined band is marked as level 23 in the data file}. Thus our Poisson model has the following explicit form: \\[\\begin{align*} \\log \\mu_i= \\mathbf{ x}^{\\prime}_i\\beta+&amp;\\log m_i=\\beta_0+\\beta_1 I(Sex_i=M)+ \\sum_{t=2}^6 \\beta_t\\, I(Vage_i=t+1) \\\\ &amp;+ \\sum_{t=7}^{13} \\beta_t \\,I(Vtype_i=A)\\times I(Age_i=t-7)+\\log m_i. \\end{align*}\\] The fitting result is given in Table 8.3, for which we have several comments. The claim frequency is higher for male by 17.3%, when other rating factors are held fixed. However, this may have been affected by the fact that all unspecified sex has been assigned to male. Regarding the vehicle age, the claim frequency gradually decreases as the vehicle gets old, when other rating factors are held fixed. The level starts from 2 for this variable but, again, the numbering is nominal and does not affect the numerical result. The policyholder age variable only applies to type A (automobile) vehicle, and there is no policy in the first age band. We may speculate that younger drivers less than age 21 drive their parents’ cars rather than having their own because of high insurance premiums or related regulations. The missing relativity may be estimated by some interpolation or the professional judgement of the actuary. The claim frequency is the lowest for age band 3 and 4, but gets substantially higher for older age bands, a reasonable pattern seen in many auto insurance loss datasets. *We also note that there is no base level in the policyholder age variable, in the sense that no relativity is equal to 1. This is because the variable is only applicable to vehicle type A. This does not cause a problem numerically, but one may set the base relativity as follows if necessary for other purposes. Since there is no policy in age band 0, we consider band 1 as the base case. Specifically, we treat its relativity as a product of 0.918 and 1, where the former is the common relativity (that is, the common premium reduction) applied to all policies with vehicle type A and the latter is the base value for age band 1. Then the relativity of age band 2 can be seen as \\(0.917=0.918 \\times 0.999\\), where 0.999 is understood as the relativity for age band 2. The remaining age bands can be treated similarly. \\[\\begin{matrix} \\begin{array}{clcc} \\hline \\text{Rating factor} &amp; \\text{Level} &amp; \\text{Relativity in the tariff} &amp; \\text{Note}\\\\ \\hline\\hline \\text{Base value} &amp; &amp; 0.167 &amp; f_0\\\\ \\hline \\text{Sex} &amp; 1 (F) &amp; 1.000 &amp; \\text{Base level}\\\\ &amp; 2 (M) &amp; 1.173 &amp;\\\\\\hline \\text{Vehicle age} &amp; 2 (0-2\\text{ yrs}) &amp; 1.000 &amp; \\text{Base level}\\\\ &amp; 3 (3-5\\text{ yrs}) &amp; 0.843 \\\\ &amp; 4 (6-10\\text{ yrs}) &amp; 0.553 \\\\ &amp; 5 (11-15\\text{ yrs}) &amp; 0.269 \\\\ &amp; 6 (16+\\text{ yrs}) &amp; 0.189 &amp;\\\\\\hline \\text{Policyholder age} &amp; 0 (0-21) &amp; \\text{N/A} &amp; \\text{No policy} \\\\ \\text{(Only applicable to} &amp; 1 (22-25) &amp; 0.918 \\\\ \\text{vehicle type A)} &amp; 2 (26-35) &amp; 0.917 \\\\ &amp; 3 (36-45) &amp; 0.758 \\\\ &amp; 4 (46-55) &amp; 0.632 \\\\ &amp; 5 (56-65) &amp; 1.102\\\\ &amp; 6 (65+) &amp; 1.179\\\\ \\hline \\hline \\end{array} \\end{matrix}\\] [Table 8.3] : Singapore insurance claims data Let us try several examples based on Table 8.3. Suppose a male policyholder aged 40 who owns a 7-year-old vehicle of type A. The expected claim frequency for this policyholder is then given by \\[\\begin{equation} \\lambda=0.167 \\times 1.173 \\times 0.553 \\times 0.758 = 0.082. \\end{equation}\\] As another example consider a female policyholder aged 60 who owns a 3-year-old vehicle of type O. The expected claim frequency for this policyholder is \\[\\begin{equation} \\lambda=0.167 \\times 1 \\times 0.843 = 0.141. \\end{equation}\\] Note that for this policy the age band variable is not used as the vehicle type is not A. The R script is given below. mydat &lt;- read.csv(&quot;SingaporeAuto.csv&quot;, quote = &quot;&quot;, header = TRUE) attach(mydat) # create vehicle type as factor TypeA = 1 * (VehicleType == &quot;A&quot;) table(VehicleType) VtypeF &lt;- as.character(VehicleType) VtypeF[VtypeF != &quot;A&quot;] &lt;- &quot;O&quot; VtypeF = relevel(factor(VtypeF), ref=&quot;A&quot;) # create gender as factor Female = 1 * (SexInsured == &quot;F&quot; ) Sex = as.character(SexInsured) Sex[Sex != &quot;F&quot;] &lt;- &quot;M&quot; SexF = relevel(factor(Sex), ref = &quot;F&quot;) # create driver age as factor AgeCat = pmax(AgeCat - 1, 0) AgeCatF = relevel(factor(AgeCat), ref = &quot;0&quot;) table(AgeCatF) # No policy in the first age band # create vehicle age as factor VAgeCatF = relevel( factor(VAgeCat), ref = &quot;0&quot; ) VAgecat1 = factor(VAgecat1, labels = c(&quot;Vage0-2&quot;, &quot;Vage3-5&quot;, &quot;Vage6-10&quot;, &quot;Vage11-15&quot;, &quot;Vage15+&quot;) ) VAgecat1F = relevel( factor(VAgecat1), ref = &quot;Vage0-2&quot; ) # Poisson reg model Pois_reg2 = glm(Clm_Count ~ SexF + TypeA:AgeCatF + VAgecat1F, offset = LNWEIGHT, poisson(link = log) ) summary(Pois_reg2) # compute relativities exp(Pois_reg2$coefficients) detach(mydat) 2.5 Further Reading and References The Poisson regression is a special member of a more general regression model class known as the generalized linear model (glm). The glm develops a unified regression framework for datasets when the response valuables are continuous, binary or discrete. The classical linear regression model with normal error is also a member of the glm. There are many standard statistical texts dealing with the glm, including (???). More accessible texts are (???), (???) and (???). For actuarial and insurance applications of the glm see (???), (???). Also, (???) discusses the glm in non-life insurance pricing context with tariff analyses. 2.6 Technical supplements – Estimating Poisson regression model Maximum likelihood estimation for individual data In the Poisson regression the varying Poisson mean is determined by parameters \\(\\beta_i\\)’s, as shown in (17). In this subsection we use the maximum likelihood method to estimate these parameters. Again, we assume that there are \\(n\\) policyholders and the \\(i\\)th policyholder is characterized by \\(\\mathbf{ x}_i=(1, x_{i1}, \\ldots, x_{ik})^{\\prime}\\) with the observed loss count \\(y_i\\). Then, from (16) and (17), the log-likelihood function of vector \\(\\beta=(\\beta_0, \\dots, \\beta_k)\\) is given by \\[\\begin{align} \\nonumber \\log L(\\beta) &amp;= l(\\beta)=\\sum^n_{i=1} \\left( -\\mu_i +y_i \\, \\log \\mu_i -\\log y_i! \\right) \\\\ &amp; = \\sum^n_{i=1} \\left( -m_i \\exp(\\mathbf{ x}^{\\prime}_i\\beta) +y_i \\,(\\log m_i+\\mathbf{ x}^{\\prime}_i\\beta) -\\log y_i! \\right) - (23) \\end{align}\\] To obtain the mle of \\(\\beta=(\\beta_0, \\ldots, \\beta_k)^{\\prime}\\), we differentiate4 \\(l(\\beta)\\) with respect to vector \\(\\beta\\) and set it to zero: \\[\\begin{equation} \\frac{\\partial}{\\partial \\beta}l(\\beta)\\Bigg{|}_{\\beta=\\mathbf{ b}}=\\sum^n_{i=1} \\left(y_i -m_i \\exp(\\mathbf{ x}^{\\prime}_i \\mathbf{ b}) \\right)\\mathbf{ x}_i=\\mathbf{ 0}. - (24) \\end{equation}\\] Numerically solving this equation system gives the mle of \\(\\beta\\), denoted by \\(\\mathbf{ b}=(b_0, b_1, \\ldots, b_k)^{\\prime}\\). Note that, as \\(\\mathbf{ x}_i=(1, x_{i1}, \\ldots, x_{ik})^{\\prime}\\) is a column vector, Equation (24) is a system of \\(k+1\\) equations with both sides written as column vectors of size \\(k+1\\). If we denote \\(\\hat{\\mu}_i=m_i \\exp(\\mathbf{ x}^{\\prime}_i \\mathbf{ b})\\), we can rewrite (24) as \\[\\begin{equation} \\sum^n_{i=1} \\left(y_i -\\hat{\\mu}_i \\right)\\mathbf{ x}_i=\\mathbf{ 0}. \\end{equation}\\] Since the solution \\(\\mathbf{ b}\\) satisfies this equation, it follows that the first among the array of \\(k+1\\) equations, corresponding to the first constant element of \\(\\mathbf{ x}_i\\), yields \\[\\begin{equation} \\sum^n_{i=1}\\left( y_i -\\hat{\\mu}_i \\right)\\times 1={ 0}, \\end{equation}\\] which implies that we must have \\[\\begin{equation} n^{-1}\\sum_{i=1}^n y_i =\\bar{y}=n^{-1}\\sum_{i=1}^n \\hat{\\mu}_i. \\end{equation}\\] This is an interesting property saying that the average of the individual losses, \\(\\bar{y}\\), is same as the average of the estimated values. That is, the sample mean is preserved under the fitted Poisson regression model. Maximum likelihood estimation for grouped data Sometimes the data is not available at the individual policy level. For example, Table 8.2 provides collective loss information for each risk class after grouping individual policies. When this is the case, \\(y_i\\) and \\(m_i\\), the quantities needed for the mle calculation in (24), are unavailable for each \\(i\\). However this does not pose a problem as long as we have the total loss counts and total exposure for each risk class. To elaborate, let us assume that there are \\(K\\) different risk classes, and further that, in the \\(k\\)th risk class, we have \\(n_k\\) policies with the total exposure \\(m_{(k)}\\) and the average loss count \\(\\bar{y}_{(k)}\\), for \\(k=1, \\ldots, K\\); the total loss count for the \\(k\\)th risk class is then \\(n_k\\, \\bar{y}_{(k)}\\). We denote the set of indices of the policies belonging to the \\(k\\)th class by \\(C_k\\). As all policies in a given risk class share the same risk characteristics, we may denote \\(\\mathbf{ x}_i=\\mathbf{ x}_{(k)}\\) for all \\(i \\in C_k\\). With this notation, we can rewrite (24) as \\[\\begin{align} \\nonumber \\sum^n_{i=1} \\left(y_i -m_i \\exp(\\mathbf{ x}^{\\prime}_i \\mathbf{ b}) \\right)\\mathbf{ x}_i &amp;= \\sum^K_{k=1}\\Big{\\{}\\sum_{i \\in C_k} \\left(y_i -m_i \\exp(\\mathbf{ x}^{\\prime}_i \\mathbf{ b}) \\right)\\mathbf{ x}_i \\Big{\\}} \\\\ \\nonumber &amp; =\\sum^K_{k=1}\\Big{\\{} \\sum_{i \\in C_k} \\left(y_i -m_i \\exp(\\mathbf{ x}^{\\prime}_{(k)} \\mathbf{ b}) \\right)\\mathbf{ x}_{(k)} \\Big{\\}} \\\\ \\nonumber &amp; =\\sum^K_{k=1}\\Big{\\{} \\Big(\\sum_{i \\in C_k}y_i -\\sum_{i \\in C_k}m_i \\exp(\\mathbf{ x}^{\\prime}_{(k)} \\mathbf{ b}) \\Big)\\mathbf{ x}_{(k)} \\Big{\\}} \\\\ &amp; =\\sum^K_{k=1} \\Big(n_k\\, \\bar{y}_{(k)}-m_{(k)} \\exp(\\mathbf{ x}^{\\prime}_{(k)} \\mathbf{ b}) \\Big)\\mathbf{ x}_{(k)} =0. - (25) \\end{align}\\] Since \\(n_k\\, \\bar{y}_{(k)}\\) in (25) represents the total loss count for the \\(k\\)th risk class and \\(m_{(k)}\\) is its total exposure, we see that for the Poisson regression the mle \\(\\mathbf{ b}\\) is the same whether if we use the individual data or the grouped data. Information matrix Taking second derivatives to (23) gives the information matrix of the mle estimators, \\[\\begin{equation} \\mathbf{ I}(\\beta)=-\\mathrm{E~}{\\left( \\frac{\\partial^2}{\\partial \\beta\\partial \\beta^{\\prime}}l(\\beta) \\right)}=\\sum^n_{i=1}m_i \\exp(\\mathbf{ x}^{\\prime}_i \\mathbf{ \\beta})\\mathbf{ x}_i \\mathbf{ x}_i^{\\prime}=\\sum^n_{i=1} {\\mu}_i \\mathbf{ x}_i \\mathbf{ x}_i^{\\prime}. - (26) \\end{equation}\\] For actual datasets, \\({\\mu}_i\\) in (26) is replaced with \\(\\hat{\\mu}_i=m_i \\exp(\\mathbf{ x}^{\\prime}_i \\mathbf{ b})\\) to estimate the relevant variances and covariances of the mle \\(\\mathbf{ b}\\) or its functions. For grouped datasets, we have \\[\\begin{equation} \\mathbf{ I}(\\beta)=\\sum^K_{k=1} \\Big{\\{}\\sum_{i \\in C_k}m_i \\exp(\\mathbf{ x}^{\\prime}_i \\mathbf{ \\beta})\\mathbf{ x}_i \\mathbf{ x}_i^{\\prime} \\Big{\\}}=\\sum^K_{k=1} m_{(k)} \\exp(\\mathbf{ x}^{\\prime}_{(k)} \\mathbf{ \\beta})\\mathbf{ x}_{(k)} \\mathbf{ x}_{(k)}^{\\prime}. \\end{equation}\\] For example, if there are 3 risk factors each of which the number of levels are 2, 3 and 4, respectively, we have \\(k=(2-1)\\times(3-1)\\times (4-1)=6\\).↩ Preferring the multiplicative form to others (e.g., additive one) was already hinted in (4).↩ corresponding to \\(\\texttt{VAgecat1}\\)↩ We use matrix derivative here.↩ "],
["data-systems.html", "Chapter 3 Data Systems 3.1 Data 3.2 Data Analysis Preliminary 3.3 Data Analysis Techniques 3.4 Some R Functions 3.5 Summary 3.6 Further Resources and Contributors", " Chapter 3 Data Systems Chapter Preview. This chapter covers the learning areas on data and systems outlined in the IAA (International Actuarial Association) Education Syllabus published in September 2015. 3.1 Data 3.1.1 Data Types and Sources In terms of how data are collected, data can be divided into two types (Hox and Boeije 2005): primary data and secondary data. Primary data are original data that are collected for a specific research problem. Secondary data are data originally collected for a different purpose and reused for another research problem. A major advantage of using primary data is that the theoretical constructs, the research design, and the data collection strategy can be tailored to the underlying research question to ensure that the data collected indeed help to solve the problem. A disadvantage of using primary data is that data collection can be costly and time-consuming. Using secondary data has the advantage of lower cost and faster access to relevant information. However, using secondary data may not be optimal for the research question under consideration. In terms of the degree of organization of the data, data can be also divided into two types (Inmon and Linstedt 2014; O’Leary 2013; Hashem et al. 2015; Abdullah and Ahmad 2013; Pries and Dunnigan 2015): structured data and unstructured data. Structured data have a predictable and regularly occurring format. In contrast, unstructured data are unpredictable and have no structure that is recognizable to a computer. Structured data consists of records, attributes, keys, and indices and are typically managed by a database management system (DBMS) such as IBM DB2, Oracle, MySQL, and Microsoft SQL Server. As a result, most units of structured data can be located quickly and easily. Unstructured data have many different forms and variations. One common form of unstructured data is text. Accessing unstructured data is clumsy. To find a given unit of data in a long text, for example, sequentially search is usually performed. In terms of how the data are measured, data can be classified as qualitative or quantitative. Qualitative data is data about qualities, which cannot be actually measured. As a result, qualitative data is extremely varied in nature and includes interviews, documents, and artifacts (Miles, Hberman, and Sdana 2014). Quantitative data is data about quantities, which can be measured numerically with numbers. In terms of the level of measurement, quantitative data can be further classified as nominal, ordinal, interval, or ratio (Gan 2011). Nominal data, also called categorical data, are discrete data without a natural ordering. Ordinal data are discrete data with a natural order. Interval data are continuous data with a specific order and equal intervals. Ratio data are interval data with a natural zero. There exist a number of data sources. First, data can be obtained from university-based researchers who collect primary data. Second, data can be obtained from organizations that are set up for the purpose of releasing secondary data for general research community. Third, data can be obtained from national and regional statistical institutes that collect data. Finally, companies have corporate data that can be obtained for research purpose. While it might be difficult to obtain data to address a specific research problem or answer a business question, it is relatively easy to obtain data to test a model or an algorithm for data analysis. In nowadays, readers can obtain datasets from the Internet easily. The following is a list of some websites to obtain real-world data: UCI Machine Learning Repository This website (url: http://archive.ics.uci.edu/ml/index.php) maintains more than 400 datasets that can be used to test machine learning algorithms. Kaggle The Kaggle website (url: https://www.kaggle.com/) include real-world datasets used for data science competition. Readers can download data from Kaggle by registering an account. DrivenData DrivenData aims at bringing cutting-edge practices in data science to solve some of the world’s biggest social challenges. In its website (url: https://www.drivendata.org/), readers can participate data science competitions and download datasets. Analytics Vidhya This website (url: https://datahack.analyticsvidhya.com/contest/all/) allows you to participate and download datasets from practice problems and hackathon problems. KDD Cup KDD Cup is the annual Data Mining and Knowledge Discovery competition organized by ACM Special Interest Group on Knowledge Discovery and Data Mining. This website (url: http://www.kdd.org/kdd-cup) contains the datasets used in past KDD Cup competitions since 1997. U.S. Government’s open data This website (url: https://www.data.gov/) contains about 200,000 datasets covering a wide range of areas including climate, education, energy, and finance. AWS Public Datasets In this website (url: https://aws.amazon.com/datasets/), Amazon provides a centralized repository of public datasets, including some huge datasets. 3.1.2 Data Structures and Storage As mentioned in the previous subsection, there are structured data as well as unstructured data. Structured data are highly organized data and usually have the following tabular format: \\[\\begin{matrix} \\begin{array}{lllll} \\hline &amp; V_1 &amp; V_2 &amp; \\cdots &amp; V_d \\ \\\\\\hline \\textbf{x}_1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1d} \\\\ \\textbf{x}_2 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2d} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ \\textbf{x}_n &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nd} \\\\ \\hline \\end{array} \\end{matrix} \\] In other words, structured data can be organized into a table consists of rows and columns. Typically, each row represents a record and each column represents an attribute. A table can be decomposed into several tables that can be stored in a relational database such as the Microsoft SQL Server. The SQL (Structured Query Language) can be used to access and modify the data easily and efficiently. Unstructured data do not follow a regular format (Abdullah and Ahmad 2013). Examples of unstructured data include documents, videos, and audio files. Most of the data we encounter are unstructured data. In fact, the term ``big data’’ was coined to reflect this fact. Traditional relational databases cannot meet the challenges on the varieties and scales brought by massive unstructured data nowadays. NoSQL databases have been used to store massive unstructured data. There are three main NoSQL databases (Chen et al. 2014): key-value databases, column-oriented databases, and document-oriented databases. Key-value databases use a simple data model and store data according to key-values. Modern key-value databases have higher expandability and smaller query response time than relational databases. Examples of key-value databases include Dynamo used by Amazon and Voldemort used by LinkedIn. Column-oriented databases store and process data according to columns rather than rows. The columns and rows are segmented in multiple nodes to achieve expandability. Examples of column-oriented databases include BigTable developed by Google and Cassandra developed by FaceBook. Document databases are designed to support more complex data forms than those stored in key-value databases. Examples of document databases include MongoDB, SimpleDB, and CouchDB. MongoDB is an open-source document-oriented database that stores documents as binary objects. SimpleDB is a distributed NoSQL database used by Amazon. CouchDB is an another open-source document-oriented database. 3.1.3 Data Quality Accurate data are essential to useful data analysis. The lack of accurate data may lead to significant costs to organizations in areas such as correction activities, lost customers, missed opportunities, and incorrect decisions (Olson 2003). Data has quality if it satisfies its intended use, that is, the data is accurate, timely, relevant, complete, understood, and trusted (Olson 2003). As a result, we first need to know the specification of the intended uses and then judge the suitability for those uses in order to assess the quality of the data. Unintended uses of data can arise from a variety of reasons and lead to serious problems. Accuracy is the single most important component of high-quality data. Accurate data have the following properties (Olson 2003): The data elements are not missing and have valid values. The values of the data elements are in the right ranges and have the right representations. Inaccurate data arise from different sources. In particular, the following areas are common areas where inaccurate data occur: Initial data entry. Mistakes (including deliberate errors) and system errors can occur during the initial data entry. Flawed data entry processes can result in inaccurate data. Data decay. Data decay, also known as data degradation, refers to the gradual corruption of computer data due to an accumulation of non-critical failures in a storage device. Data moving and restructuring. Inaccurate data can also arise from data extracting, cleaning, transforming, loading, or integrating. Data using. Faulty reporting and lack of understanding can lead to inaccurate data. Reverification and analysis are two approaches to find inaccurate data elements. To ensure that the data elements are 100% accurate, we must use reverification. However, reverification can be time-consuming and may not be possible for some data. Analytical techniques can also be used to identify inaccurate data elements. There are five types of analysis that can be used to identify inaccurate data (Olson 2003): data element analysis, structural analysis, value correlation, aggregation correlation, and value inspection Companies can create a data quality assurance program to create high-quality databases. For more information about data quality issues management and data profiling techniques, readers are referred to (Olson 2003). 3.1.4 Data Cleaning Raw data usually need to be cleaned before useful analysis can be conducted. In particular, the following areas need attention when preparing data for analysis (Janert 2010): Missing values It is common to have missing values in raw data. Depending on the situations, we can discard the record, discard the variable, or impute the missing values. Outliers Raw data may contain unusual data points such as outliers. We need to handle outliers carefully. We cannot just remove outliers without knowing the reason for their existence. Sometimes the outliers are caused by clerical errors. Sometimes outliers are the effect we are looking for. Junk Raw data may contain junks such as nonprintable characters. Junks are typically rare and not easy to get noticed. However, junks can cause serious problems in downstream applications. Format Raw data may be formated in a way that is inconvenient for subsequent analysis. For example, components of a record may be split into multiple lines in a text file. In such cases, lines corresponding to a single record should be merged before loading to a data analysis software such as R. Duplicate records Raw data may contain duplicate records. Duplicate records should be recognized and removed. This task may not be trivial depending on what you consider ``duplicate.’’ Merging datasets Raw data may come from different sources. In such cases, we need to merge the data from different sources to ensure compatibility. For more information about how to handle data in R, readers are referred to (Forte 2015) and (Buttrey and Whitaker 2017). 3.2 Data Analysis Preliminary Data analysis involves inspecting, cleansing, transforming, and modeling data to discover useful information to suggest conclusions and make decisions. Data analysis has a long history. In 1962, statistician John Tukey defined data analysis as (Tukey 1962): procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. Recently, Judd and coauthors defined data analysis as the following equation(Judd, McClelland, and Ryan 2017): \\[\\hbox{Data} = \\hbox{Model} + \\hbox{Error},\\] where Data represents a set of basic scores or observations to be analyzed, Model is a compact representation of the data, and Error is simply the amount the model fails to represent accurately. Using the above equation for data analysis, an analyst must resolve the following two conflicting goals: to add more parameters to the model so that the model represents the data better. to remove parameters from the model so that the model is simple and parsimonious. In this section, we give a high-level introduction to data analysis, including different types of methods. 3.2.1 Data Analysis Process Data analysis is part of an overall study. For example, Figure 3.1 shows the process of a typical study in behavioral and social sciences as described in (Albers 2017). The data analysis part consists of the following steps: Exploratory analysis The purpose of this step is to get a feel of the relationships with the data and figure out what type of analysis for the data makes sense. Statistical analysis This step performs statistical analysis such as determining statistical significance and effect size. Make sense of the results This step interprets the statistical results in the context of the overall study. Determine implications This step interprets the data by connecting it to the study goals and the larger field of this study. The goal of the data analysis as described above focuses on explaining some phenomenon (See Section 3.2.5). Figure 3.1: The process of a typical study in behavioral and social sciences. Shmueli (2010) described a general process for statistical modeling, which is shown in Figure 3.2. Depending on the goal of the analysis, the steps differ in terms of the choice of methods, criteria, data, and information. Figure 3.2: The process of statistical modeling. 3.2.2 Exploratory versus Confirmatory There are two phases of data analysis (Good 1983): exploratory data analysis (EDA) and confirmatory data analysis (CDA). Table 1.1 summarizes some differences between EDA and CDA. EDA is usually applied to observational data with the goal of looking for patterns and formulating hypotheses. In contrast, CDA is often applied to experimental data (i.e., data obtained by means of a formal design of experiments) with the goal of quantifying the extent to which discrepancies between the model and the data could be expected to occur by chance (Gelman 2004). \\[\\begin{matrix} \\begin{array}{lll} \\hline &amp; \\textbf{EDA} &amp; \\textbf{CDA} \\\\\\hline \\text{Data} &amp; \\text{Observational data} &amp; \\text{Experimental data}\\\\[3mm] \\text{Goal} &amp; \\text{Pattern recognition,} &amp; \\text{Hypothesis testing,} \\\\ &amp; \\text{formulate hypotheses} &amp; \\text{estimation, prediction} \\\\[3mm] \\text{Techniques} &amp; \\text{Descriptive statistics,} &amp; \\text{Traditional statistical tools of} \\\\ &amp; \\text{visualization, clustering} &amp; \\text{inference, significance, and}\\\\ &amp; &amp; \\text{confidence} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 1.1: Comparison of exploratory data analysis and confirmatory data analysis. Techniques for EDA include descriptive statistics (e.g., mean, median, standard deviation, quantiles), distributions, histograms, correlation analysis, dimension reduction, and cluster analysis. Techniques for CDA include the traditional statistical tools of inference, significance, and confidence. 3.2.3 Supervised versus Unsupervised Methods for data analysis can be divided into two types (Abbott 2014; Igual and Segu 2017): supervised learning methods and unsupervised learning methods. Supervised learning methods work with labeled data, which include a target variable. Mathematically, supervised learning methods try to approximate the following function: \\[ Y = f(X_1, X_2, \\ldots, X_p), \\] where \\(Y\\) is a target variable and \\(X_1\\), \\(X_2\\), \\(\\ldots\\), \\(X_p\\) are explanatory variables. Other terms are also used to mean a target variable. Table 1.2 gives a list of common names for different types of variables (Frees 2009). When the target variable is a categorical variable, supervised learning methods are called classification methods. When the target variable is continuous, supervised learning methods are called regression methods. \\[\\begin{matrix} \\begin{array}{ll} \\hline \\textbf{Target Variable} &amp; \\textbf{Explanatory Variable}\\\\\\hline \\text{Dependent variable} &amp; \\text{Independent variable}\\\\ \\text{Response} &amp; \\text{Treatment} \\\\ \\text{Output} &amp; \\text{Input} \\\\ \\text{Endogenous variable} &amp; \\text{Exogenous variable} \\\\ \\text{Predicted variable} &amp; \\text{Predictor variable} \\\\ \\text{Regressand} &amp; \\text{Regressor} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 1.2: Common names of different variables. Unsupervised learning methods work with unlabeled data, which include explanatory variables only. In other words, unsupervised learning methods do not use target variables. As a result, unsupervised learning methods are also called descriptive modeling methods. 3.2.4 Parametric versus Nonparametric Methods for data analysis can be parametric or nonparametric (Abbott 2014). Parametric methods assume that the data follow a certain distribution. Nonparametric methods do not assume distributions for the data and therefore are called distribution-free methods. Parametric methods have the advantage that if the distribution of the data is known, properties of the data and properties of the method (e.g., errors, convergence, coefficients) can be derived. A disadvantage of parametric methods is that analysts need to spend considerable time on figuring out the distribution. For example, analysts may try different transformation methods to transform the data so that it follows a certain distribution. Since nonparametric methods make fewer assumptions, nonparametric methods have the advantage that they are more flexible, more robust, and applicable to non-quantitative data. However, a drawback of nonparametric methods is that the conclusions drawn from nonparametric methods are not as powerful as those drawn from parametric methods. 3.2.5 Explanation versus Prediction There are two goals in data analysis (Breiman 2001; Shmueli 2010): explanation and prediction. In some scientific areas such as economics, psychology, and environmental science, the focus of data analysis is to explain the causal relationships between the input variables and the response variable. In other scientific areas such as natural language processing and bioinformatics, the focus of data analysis is to predict what the responses are going to be given the input variables. Shmueli (2010) discussed in detail the distinction between explanatory modeling and predictive modeling, which reflect the process of using data and methods for explaining or predicting, respectively. Explanatory modeling is commonly used for theory building and testing. However, predictive modeling is rarely used in many scientific fields as a tool for developing theory. Explanatory modeling is typically done as follows: State the prevailing theory. State causal hypotheses, which are given in terms of theoretical constructs rather than measurable variables. A causal diagram is usually included to illustrate the hypothesized causal relationship between the theoretical constructs. Operationalize constructs. In this step, previous literature and theoretical justification are used to build a bridge between theoretical constructs and observable measurements. Collect data and build models alongside the statistical hypotheses, which are operationalized from the research hypotheses. Reach research conclusions and recommend policy. The statistical conclusions are converted into research conclusions. Policy recommendations are often accompanied. Shmueli (2010) defined predictive modeling as the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations. Predictions include point predictions, interval predictions, regions, distributions, and rankings of new observations. Predictive model can be any method that produces predictions. 3.2.6 Data Modeling versus Algorithmic Modeling Breiman (2001) discussed two cultures in the use of statistical modeling to reach conclusions from data: the data modeling culture and the algorithmic modeling culture. In the data modeling culture, the data are assumed to be generated by a given stochastic data model. In the algorithmic modeling culture, the data mechanism is treated as unknown and algorithmic models are used. Data modeling gives the statistics field many successes in analyzing data and getting information about the data mechanisms. However, Breiman (2001) argued that the focus on data models in the statistical community has led to some side effects such as Produced irrelevant theory and questionable scientific conclusions. Kept statisticians from using algorithmic models that might be more suitable. Restricted the ability of statisticians to deal with a wide range of problems. Algorithmic modeling was used by industrial statisticians long time ago. However, the development of algorithmic methods was taken up by a community outside statistics (Breiman 2001). The goal of algorithmic modeling is predictive accuracy. For some complex prediction problems, data models are not suitable. These prediction problems include speech recognition, image recognition, handwriting recognition, nonlinear time series prediction, and financial market prediction. The theory in algorithmic modeling focuses on the properties of algorithms, such as convergence and predictive accuracy. 3.2.7 Big Data Analysis Unlike traditional data analysis, big data analysis employs additional methods and tools that can extract information rapidly from massive data. In particular, big data analysis uses the following processing methods (Chen et al. 2014): Bloom filter A bloom filter is a space-efficient probabilistic data structure that is used to determine whether an element belongs to a set. It has the advantages of high space efficiency and high query speed. A drawback of using bloom filter is that there is a certain misrecognition rate. Hashing Hashing is a method that transforms data into fixed-length numerical values through a hash function. It has the advantages of rapid reading and writing. However, sound hash functions are difficult to find. Indexing Indexing refers to a process of partitioning data in order to speed up reading. Hashing is a special case of indexing. Tries A trie, also called digital tree, is a method to improve query efficiency by using common prefixes of character strings to reduce comparison on character strings to the greatest extent. Parallel computing Parallel computing uses multiple computing resources to complete a computation task. Parallel computing tools include MPI (Message Passing Interface), MapReduce, and Dryad. Big data analysis can be conducted in the following levels (Chen et al. 2014): memory-level, business intelligence (BI) level, and massive level. Memory-level analysis is conducted when the data can be loaded to the memory of a cluster of computers. Current hardware can handle hundreds of gigabytes (GB) of data in memory. BI level analysis can be conducted when the data surpass the memory level. It is common for BI level analysis products to support data over terabytes (TB). Massive level analysis is conducted when the data surpass the capabilities of products for BI level analysis. Usually Hadoop and MapReduce are used in massive level analysis. 3.2.8 Reproducible Analysis As mentioned in Section 3.2.1, a typical data analysis workflow includes collecting data, analyzing data, and reporting results. The data collected are saved in a database or files. The data are then analyzed by one or more scripts, which may save some intermediate results or always work on the raw data. Finally a report is produced to describe the results, which include relevant plots, tables, and summaries of the data. The workflow may subject to the following potential issues (Mailund 2017, Chapter 2): The data are separated from the analysis scripts. The documentation of the analysis is separated from the analysis itself. If the analysis is done on the raw data with a single script, then the first issue is not a major problem. If the analysis consists of multiple scripts and a script saves intermediate results that are read by the next script, then the scripts describe a workflow of data analysis. To reproduce an analysis, the scripts have to be executed in the right order. The workflow may cause major problems if the order of the scripts is not documented or the documentation is not updated or lost. One way to address the first issue is to write the scripts so that any part of the workflow can be run completely automatically at any time. If the documentation of the analysis is synchronized with the analysis, then the second issue is not a major problem. However, the documentation may become completely useless if the scripts are changed but the documentation is not updated. Literate programming is an approach to address the two issues mentioned above. In literate programming, the documentation of a program and the code of the program are written together. To do literate programming in R, one way is to use the R Markdown and the \\(\\texttt{knitr}\\) package. 3.2.9 Ethical Issues Analysts may face ethical issues and dilemmas during the data analysis process. In some fields, for example, ethical issues and dilemmas include participant consent, benefits, risk, confidentiality, and data ownership (Miles, Hberman, and Sdana 2014). For data analysis in actuarial science and insurance in particular, we face the following ethical matters and issues (Miles, Hberman, and Sdana 2014): Worthness of the project Is the project worth doing? Will the project contribute in some significant way to a domain broader than my career? If a project is only opportunistic and does not have a larger significance, then it might be pursued with less care. The result may be looked good but not right. Competence Do I or the whole team have the expertise to carry out the project? Incompetence may lead to weakness in the analytics such as collecting large amounts of data poorly and drawing superficial conclusions. Benefits, costs, and reciprocity Will each stakeholder gain from the project? Are the benefit and the cost equitable? A project will likely to fail if the benefit and the cost for a stakeholder do not match. Privacy and confidentiality How do we make sure that the information is kept confidentially? Where raw data and analysis results are stored and how will have access to them should be documented in explicit confidentiality agreements. 3.3 Data Analysis Techniques Techniques for data analysis are drawn from different but overlapping fields such as statistics, machine learning, pattern recognition, and data mining. Statistics is a field that addresses reliable ways of gathering data and making inferences based on them (Bandyopadhyay and Forster 2011; Bluman 2012). The term machine learning was coined by Samuel in 1959 (Samuel 1959). Originally, machine learning refers to the field of study where computers have the ability to learn without being explicitly programmed. Nowadays, machine learning has evolved to the broad field of study where computational methods use experience (i.e., the past information available for analysis) to improve performance or to make accurate predictions (Bishop 2007; Clarke, Fokoue, and Zhang 2009; Mohri, Rostamizadeh, and Talwalkar 2012; Kubat 2017). There are four types of machine learning algorithms (See Table 1.3 depending on the type of the data and the type of the learning tasks. \\[\\begin{matrix} \\begin{array}{rll} \\hline &amp; \\textbf{Supervised} &amp; \\textbf{Unsupervised} \\\\\\hline \\textbf{Discrete Label} &amp; \\text{Classification} &amp; \\text{Clustering} \\\\ \\textbf{Continuous Label} &amp; \\text{Regression} &amp; \\text{Dimension reduction} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 1.3: Types of machine learning algorithms. Originating in engineering, pattern recognition is a field that is closely related to machine learning, which grew out of computer science. In fact, pattern recognition and machine learning can be considered to be two facets of the same field (Bishop 2007). Data mining is a field that concerns collecting, cleaning, processing, analyzing, and gaining useful insights from data (Aggarwal 2015). 3.3.1 Exploratory Techniques Exploratory data analysis techniques include descriptive statistics as well as many unsupervised learning techniques such as data clustering and principal component analysis. 3.3.2 Descriptive Statistics In the mass noun sense, descriptive statistics is an area of statistics that concerns the collection, organization, summarization, and presentation of data (Bluman 2012). In the count noun sense, descriptive statistics are summary statistics that quantitatively describe or summarize data. \\[\\begin{matrix} \\begin{array}{ll} \\hline &amp; \\textbf{Descriptive Statistics} \\\\\\hline \\text{Measures of central tendency} &amp; \\text{Mean, median, mode, midrange}\\\\ \\text{Measures of variation} &amp; \\text{Range, variance, standard deviation} \\\\ \\text{Measures of position} &amp; \\text{Quantile} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 1.4: Some commonly used descriptive statistics. Table 1.4 lists some commonly used descriptive statistics. In R, we can use the function \\(\\texttt{summary}\\) to calculate some of the descriptive statistics. For numeric data, we can visualize the descriptive statistics using a boxplot. In addition to these quantitative descriptive statistics, we can also qualitatively describe shapes of the distributions (Bluman 2012). For example, we can say that a distribution is positively skewed, symmetric, or negatively skewed. To visualize the distribution of a variable, we can draw a histogram. 3.3.2.1 Principal Component Analysis Principal component analysis (PCA) is a statistical procedure that transforms a dataset described by possibly correlated variables into a dataset described by linearly uncorrelated variables, which are called principal components and are ordered according to their variances. PCA is a technique for dimension reduction. If the original variables are highly correlated, then the first few principal components can account for most of the variation of the original data. To describe PCA, let \\(X_1,X_2,\\ldots,X_d\\) be a set of variables. The first principal component is defined to be the normalized linear combination of the variables that has the largest variance, that is, the first principal component is defined as \\[Z_1=w_{11} X_1 + w_{12} X_2 + \\cdots + w_{1d} X_d,\\] where \\(\\textbf{w}_1=(w_{11}, w_{12}, \\ldots, w_{1d})&#39;\\) is a vector of loadings such that \\(\\mathrm{Var~}{(Z_1)}\\) is maximized subject to the following constraint: \\[\\textbf{w}_1&#39;\\textbf{w}_1 = \\sum_{j=1}^d w_{1j}^2 = 1.\\] For \\(i=2,3,\\ldots,d\\), the \\(i\\)th principal component is defined as \\[Z_i=w_{i1} X_1 + w_{i2} X_2 + \\cdots + w_{id} X_d,\\] where \\(\\textbf{w}_i=(w_{i1}, w_{i2}, \\ldots, w_{id})&#39;\\) is a vector of loadings such that \\(\\mathrm{Var~}{(Z_i)}\\) is maximized subject to the following constraints: \\[\\textbf{w}_i&#39;\\textbf{w}_i=\\sum_{j=1}^d w_{ij}^2 = 1,\\] \\[\\mathrm{cov~}{(Z_i, Z_j)} = 0,\\quad j=1,2,\\ldots,i-1.\\] The principal components of the variables are related to the eigenvectors and eigenvectors of the covariance matrix of the variables. For \\(i=1,2,\\ldots,d\\), let \\((\\lambda_i, \\textbf{e}_i)\\) be the \\(i\\)th eigenvalue-eigenvector pair of the covariance matrix \\({\\Sigma}\\) such that \\(\\lambda_1\\ge \\lambda_2\\ge \\ldots\\ge \\lambda_d\\ge 0\\) and the eigenvectors are normalized. Then the \\(i\\)th principal component is given by \\[Z_{i} = \\textbf{e}_i&#39; \\textbf{X} =\\sum_{j=1}^d e_{ij} X_j,\\] where \\(\\textbf{X}=(X_1,X_2,\\ldots,X_d)&#39;\\). It can be shown that \\(\\mathrm{Var~}{(Z_i)} = \\lambda_i\\). As a result, the proportion of variance explained by the \\(i\\)th principal component is calculated as \\[\\dfrac{\\mathrm{Var~}{(Z_i)}}{ \\sum_{j=1}^{d} \\mathrm{Var~}{(Z_j)}} = \\dfrac{\\lambda_i}{\\lambda_1+\\lambda_2+\\cdots+\\lambda_d}.\\] For more information about PCA, readers are referred to (Mirkin 2011). 3.3.3 Cluster Analysis Cluster analysis (aka data clustering) refers to the process of dividing a dataset into homogeneous groups or clusters such that points in the same cluster are similar and points from different clusters are quite distinct (Gan, Ma, and Wu 2007; Gan 2011). Data clustering is one of the most popular tools for exploratory data analysis and has found applications in many scientific areas. During the past several decades, many clustering algorithms have been proposed. Among these clustering algorithms, the \\(k\\)-means algorithm is perhaps the most well-known algorithm due to its simplicity. To describe the \\(k\\)-means algorithm, let \\(X=\\{\\textbf{x}_1,\\textbf{x}_2,\\ldots,\\textbf{x}_n\\}\\) be a dataset containing \\(n\\) points, each of which is described by \\(d\\) numerical features. Given a desired number of clusters \\(k\\), the \\(k\\)-means algorithm aims at minimizing the following objective function: \\[P(U,Z) = \\sum_{l=1}^k\\sum_{i=1}^n u_{il} \\Vert \\textbf{x}_i-\\textbf{z}_l\\Vert^2,\\] where \\(U=(u_{il})_{n\\times k}\\) is an \\(n\\times k\\) partition matrix, \\(Z=\\{\\textbf{z}_1,\\textbf{z}_2,\\ldots,\\textbf{z}_k\\}\\) is a set of cluster centers, and \\(\\Vert\\cdot\\Vert\\) is the \\(L^2\\) norm or Euclidean distance. The partition matrix \\(U\\) satisfies the following conditions: \\[u_{il}\\in \\{0,1\\},\\quad i=1,2,\\ldots,n,\\:l=1,2,\\ldots,k,\\] \\[\\sum_{l=1}^k u_{il}=1,\\quad i=1,2,\\ldots,n.\\] The \\(k\\)-means algorithm employs an iterative procedure to minimize the objective function. It repeatedly updates the partition matrix \\(U\\) and the cluster centers \\(Z\\) alternately until some stop criterion is met. When the cluster centers \\(Z\\) are fixed, the partition matrix \\(U\\) is updated as follows: \\[\\begin{aligned}u_{il}=\\left\\{ \\begin{array}{ll} 1, &amp; \\text{if } \\Vert \\textbf{x}_i - \\textbf{z}_l\\Vert = \\min_{1\\le j\\le k} \\Vert \\textbf{x}_i - \\textbf{z}_j\\Vert;\\\\ 0, &amp; \\text{if otherwise,} \\end{array} \\right. \\end{aligned}\\] When the partition matrix \\(U\\) is fixed, the cluster centers are updated as follows: \\[z_{lj} = \\dfrac{\\sum_{i=1}^n u_{il} x_{ij} } { \\sum_{i=1}^n u_{il}},\\quad l=1,2,\\ldots,k,\\: j=1,2,\\ldots,d,\\] where \\(z_{lj}\\) is the \\(j\\)th component of \\(\\textbf{z}_l\\) and \\(x_{ij}\\) is the \\(j\\)th component of \\(\\textbf{x}_i\\). For more information about \\(k\\)-means, readers are referred to (Gan, Ma, and Wu 2007) and (Mirkin 2011). 3.3.4 Confirmatory Techniques Confirmatory data analysis techniques include the traditional statistical tools of inference, significance, and confidence. 3.3.4.1 Linear Models Linear models, also called linear regression models, aim at using a linear function to approximate the relationship between the dependent variable and independent variables. A linear regression model is called a simple linear regression model if there is only one independent variable. When more than one independent variables are involved, a linear regression model is called a multiple linear regression model. Let \\(X\\) and \\(Y\\) denote the independent and the dependent variables, respectively. For \\(i=1,2,\\ldots,n\\), let \\((x_i, y_i)\\) be the observed values of \\((X,Y)\\) in the \\(i\\)th case. Then the simple linear regression model is specified as follows (Frees 2009): \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,2,\\ldots,n,\\] where \\(\\beta_0\\) and \\(\\beta_1\\) are parameters and \\(\\epsilon_i\\) is a random variable representing the error for the \\(i\\)th case. When there are multiple independent variables, the following multiple linear regression model is used: \\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} + \\epsilon_i,\\] where \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_k\\) are unknown parameters to be estimated. Linear regression models usually make the following assumptions: \\(x_{i1},x_{i2},\\ldots,x_{ik}\\) are nonstochastic variables. \\(\\mathrm{Var~}(y_i)=\\sigma^2\\), where \\(\\mathrm{Var~}(y_i)\\) denotes the variance of \\(y_i\\). \\(y_1,y_2,\\ldots,y_n\\) are independent random variables. For the purpose of obtaining tests and confidence statements with small samples, the following strong normality assumption is also made: \\(\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n\\) are normally distributed. 3.3.4.2 Generalized Linear Models The generalized linear model (GLM) is a wide family of regression models that include linear regression models as special cases. In a GLM, the mean of the response (i.e., the dependent variable) is assumed to be a function of linear combinations of the explanatory variables, i.e., \\[\\mu_i = E[y_i],\\] \\[\\eta_i = \\textbf{x}_i&#39;\\boldsymbol{\\beta} = g(\\mu_i),\\] where \\(\\textbf{x}_i=(1,x_{i1}, x_{i2}, \\ldots, x_{ik})&#39;\\) is a vector of regressor values, \\(\\mu_i\\) is the mean response for the \\(i\\)th case, and \\(\\eta_i\\) is a systematic component of the GLM. The function \\(g(\\cdot)\\) is known and is called the link function. The mean response can vary by observations by allowing some parameters to change. However, the regression parameters \\(\\boldsymbol{\\beta}\\) are assumed to be the same among different observations. GLMs make the following assumptions: \\(x_{i1},x_{i2},\\ldots,x_{in}\\) are nonstochastic variables. \\(y_1,y_2,\\ldots,y_n\\) are independent. The dependent variable is assumed to follow a distribution from the linear exponential family. The variance of the dependent variable is not assumed to be constant but is a function of the mean, i.e., \\[\\mathrm{Var~}{(y_i)} = \\phi \\nu(\\mu_i),\\] where \\(\\phi\\) denotes the dispersion parameter and \\(\\nu(\\cdot)\\) is a function. As we can see from the above specification, the GLM provides a unifying framework to handle different types of dependent variables, including discrete and continuous variables. For more information about GLMs, readers are referred to (Jong and Heller 2008) and (Frees 2009). 3.3.4.3 Tree-based Models Decision trees, also known as tree-based models, involve dividing the predictor space (i.e., the space formed by independent variables) into a number of simple regions and using the mean or the mode of the region for prediction (Breiman et al. 1984). There are two types of tree-based models: classification trees and regression trees. When the dependent variable is categorical, the resulting tree models are called classification trees. When the dependent variable is continuous, the resulting tree models are called regression trees. The process of building classification trees is similar to that of building regression trees. Here we only briefly describe how to build a regression tree. To do that, the predictor space is divided into non-overlapping regions such that the following objective function \\[f(R_1,R_2,\\ldots,R_J) = \\sum_{j=1}^J \\sum_{i=1}^n I_{R_j}(\\textbf{x}_i)(y_i - \\mu_j)^2\\] is minimized, where \\(I\\) is an indicator function, \\(R_j\\) denotes the set of indices of the observations that belong to the \\(j\\)th box, \\(\\mu_j\\) is the mean response of the observations in the \\(j\\)th box, \\(\\textbf{x}_i\\) is the vector of predictor values for the \\(i\\)th observation, and \\(y_i\\) is the response value for the \\(i\\)th observation. In terms of predictive accuracy, decision trees generally do not perform to the level of other regression and classification models. However, tree-based models may outperform linear models when the relationship between the response and the predictors is nonlinear. For more information about decision trees, readers are referred to (Breiman et al. 1984) and (Mitchell 1997). 3.4 Some R Functions R is an open-source software for statistical computing and graphics. The R software can be downloaded from the R project website at . In this section, we give some R function for data analysis, especially the data analysis tasks mentioned in previous sections. \\[\\begin{matrix} \\begin{array}{lll} \\hline \\text{Data Analysis Task} &amp; \\text{R package} &amp; \\text{R Function} \\\\\\hline \\text{Descriptive Statistics} &amp; \\texttt{base} &amp; \\texttt{summary}\\\\ \\text{Principal Component Analysis} &amp; \\texttt{stats} &amp; \\texttt{prcomp} \\\\ \\text{Data Clustering} &amp; \\texttt{stats} &amp; \\texttt{kmeans}, \\texttt{hclust} \\\\ \\text{Fitting Distributions} &amp; \\texttt{MASS} &amp; \\texttt{fitdistr} \\\\ \\text{Linear Regression Models} &amp; \\texttt{stats} &amp; \\texttt{lm} \\\\ \\text{Generalized Linear Models} &amp; \\texttt{stats} &amp; \\texttt{glm} \\\\ \\text{Regression Trees} &amp; \\texttt{rpart} &amp; \\texttt{rpart} \\\\ \\text{Survival Analysis} &amp; \\texttt{survival} &amp; \\texttt{survfit} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 1.5: Some R functions for data analysis. Table 1.5 lists a few R functions for different data analysis tasks. Readers can read the R documentation for examples of using these functions. There are also other R functions from other packages to do similar things. However, the functions listed in this table provide good start points for readers to conduct data analysis in R. For analyzing large datasets in R in an efficient way, readers are referred to (Daroczi 2015). 3.5 Summary In this chapter, we gave a high-level overview of data analysis. The overview is divided into three major parts: data, data analysis, and data analysis techniques. In the first part, we introduced data types, data structures, data storages, and data sources. In particular, we provided several websites where readers can obtain real-world datasets to horn their data analysis skills. In the second part, we introduced the process of data analysis and various aspects of data analysis. In the third part, we introduced some commonly used techniques for data analysis. In addition, we listed some R packages and functions that can be used to perform various data analysis tasks. 3.6 Further Resources and Contributors "]
]
